
================================================================================
File: doc_parser/__init__.py
================================================================================

"""Document Parser Library.

A modular, extensible document parsing library supporting multiple formats.
"""

__version__ = "0.1.0"

# ---------------------------------------------------------------------------
# Ensure all built-in parsers are imported so they self-register with
# AppConfig. This lets users simply `import doc_parser` and immediately
# call `AppConfig.get_parser(...)` without manually importing each parser
# module first.
# ---------------------------------------------------------------------------
from doc_parser.config import AppConfig

from . import parsers as _builtin_parsers  # noqa: F401 unused-import
from .core.base import BaseParser, ParseResult
from .core.exceptions import ConfigurationError, ParserError

__all__ = [
    "AppConfig",
    "BaseParser",
    "ConfigurationError",
    "ParseResult",
    "ParserError",
]



================================================================================
File: doc_parser/cli.py
================================================================================

"""doc_parser command-line interface (CLI).

This CLI enables parsing of documents (PDF, DOCX, Excel, HTML, PPTX) and URLs
with optional caching and LLM-based post-processing.

Commands:
  parse   Parse a document or URL and output in markdown or JSON.

Examples:
  >>> python -m doc_parser.cli parse sample.pdf -f markdown
  >>> python -m doc_parser.cli parse sample.docx --format json --no-cache -o result.json
  >>> python -m doc_parser.cli parse https://example.com --post-prompt "Summarize content"
"""

from __future__ import annotations

import asyncio
from pathlib import Path  # required at runtime
from typing import Any, cast

import typer

from .config import AppConfig
from .options import PdfOptions

# For type checking only (avoid reimport warnings)

app = typer.Typer(add_completion=False, help="Document parser CLI")

# Add module-level Typer argument and option defaults to avoid function calls in defaults (B008)
FILE_ARG = typer.Argument(..., exists=True, readable=True, help="Input document or URL to parse")
FORMAT_OPTION = typer.Option("markdown", "--format", "-f", help="Output format: markdown or json")
NO_CACHE_OPTION = typer.Option(False, "--no-cache", help="Disable cache for this run")
POST_PROMPT_OPTION = typer.Option(None, "--post-prompt", help="LLM prompt for post-processing")
OUTPUT_OPTION = typer.Option(None, "--output", "-o", help="File path to save output instead of stdout")

# PDF-specific options
PAGE_RANGE_OPTION = typer.Option(
    None,
    "--page-range",
    help="For PDFs: inclusive 1-based page range, e.g. '1:3'",
)
PROMPT_TEMPLATE_OPTION = typer.Option(
    None,
    "--prompt-template",
    help="Custom prompt template (string or template ID) for extraction (PDF only).",
)

# New configuration helpers
CONFIG_FILE_OPTION = typer.Option(
    None,
    "--config-file",
    "-c",
    exists=True,
    readable=True,
    help="Optional JSON/TOML/YAML file with Settings overrides",
)

_ENV_CONFIG_PATH = "DOC_PARSER_CONFIG"


def _load_config_file(path: Path | None) -> dict[str, Any]:
    """Return dictionary from *path* (JSON / TOML / YAML)."""
    import json
    from pathlib import Path as _Path

    if path is None:
        return {}

    path = _Path(path)
    if not path.exists():
        raise FileNotFoundError(path)

    suffix = path.suffix.lower()
    if suffix in {".json"}:
        return cast("dict[str, Any]", json.loads(path.read_text()))
    if suffix in {".toml"}:
        try:
            import tomllib as _toml
        except ModuleNotFoundError as exc:  # pragma: no cover
            raise RuntimeError("TOML support requires Python 3.11+ or 'tomli' package") from exc
        return _toml.loads(path.read_text())
    if suffix in {".yaml", ".yml"}:
        try:
            import yaml as _yaml  # PyYAML
        except ModuleNotFoundError as exc:  # pragma: no cover
            raise RuntimeError("YAML support requires 'pyyaml' package") from exc
        loaded = _yaml.safe_load(path.read_text()) or {}
        if not isinstance(loaded, dict):
            raise TypeError("YAML config must represent a mapping at top-level")
        return cast("dict[str, Any]", loaded)
    raise ValueError("Unsupported config file extension (use .json, .toml, .yaml)")


@app.command()
def parse(
    file: Path = FILE_ARG,
    format: str = FORMAT_OPTION,
    no_cache: bool = NO_CACHE_OPTION,
    post_prompt: str | None = POST_PROMPT_OPTION,
    output: Path | None = OUTPUT_OPTION,
    page_range: str | None = PAGE_RANGE_OPTION,
    prompt_template: str | None = PROMPT_TEMPLATE_OPTION,
    config_file: Path | None = CONFIG_FILE_OPTION,
) -> None:
    """Parse a document or URL and print or save the result.

    Determines the appropriate parser based on file extension or URL,
    applies caching and optional post-processing, and outputs content.

    Args:
        file (Path): Input document path or URL.
        format (str): 'markdown' or 'json'.
        no_cache (bool): If True, disables caching.
        post_prompt (Optional[str]): LLM prompt for post-processing.
        output (Optional[Path]): Destination file path for saving result.
        page_range (Optional[str]): For PDFs: inclusive 1-based page range, e.g. '1:3'.
        prompt_template (Optional[str]): Custom prompt template (string or template ID) for extraction (PDF only).
        config_file (Optional[Path]): Optional JSON/TOML/YAML file with Settings overrides.

    Examples:
        >>> # Parse PDF to markdown and print
        >>> python -m doc_parser.cli parse sample.pdf -f markdown
        >>> # Parse DOCX to JSON without cache and save
        >>> python -m doc_parser.cli parse doc.docx --format json --no-cache -o result.json
        >>> # Parse URL with a post-processing prompt
        >>> python -m doc_parser.cli parse example.url --post-prompt "Summarize content"
    """
    # ------------------------------------------------------------------
    # Merge CLI / env / file overrides into Settings
    # ------------------------------------------------------------------
    import os

    # 1) Load from --config-file or env path
    file_cfg: dict[str, Any] = {}
    cfg_path: Path | None = config_file
    if cfg_path is None and (env_val := os.getenv(_ENV_CONFIG_PATH)):
        cfg_path = Path(env_val)
    if cfg_path is not None:
        file_cfg = _load_config_file(cfg_path)

    # 2) CLI overrides always win
    cli_overrides: dict[str, Any] = {
        "output_format": format,
        "use_cache": not no_cache,
        "post_prompt": post_prompt,
    }
    # Remove None values so they don't override file settings
    cli_overrides = {k: v for k, v in cli_overrides.items() if v is not None}

    merged_cfg = {**file_cfg, **cli_overrides}

    settings = AppConfig(**merged_cfg)

    parser = AppConfig.from_path(file, settings)

    # ------------------------------------------------------------------
    # Build typed options object based on parser type
    # ------------------------------------------------------------------
    options_obj: Any | None = None
    from doc_parser.parsers.pdf.parser import PDFParser  # local import to avoid heavy deps on startup

    if isinstance(parser, PDFParser):
        # Page range conversion if provided
        pr: tuple[int, int] | None = None
        if page_range:
            try:
                parts = page_range.split(":")
                if len(parts) == 2:  # noqa: PLR2004
                    pr = (int(parts[0]), int(parts[1]))
                else:
                    raise ValueError
            except ValueError as exc:  # pragma: no cover
                typer.echo("--page-range must be of form START:END", err=True)
                raise typer.Exit(1) from exc

        options_obj = PdfOptions(page_range=pr, prompt_template=prompt_template)

    # Execute asynchronous parse via asyncio.run for CLI convenience
    result = asyncio.run(parser.parse(file, options=options_obj))

    if output:
        output.parent.mkdir(parents=True, exist_ok=True)
        output.write_text(result.content, encoding="utf-8")
        typer.echo(f"Saved output to {output}")
    else:
        typer.echo(result.content)


if __name__ == "__main__":  # pragma: no cover
    app()



================================================================================
File: doc_parser/config.py
================================================================================

"""Application-wide configuration & parser registry.

This module replaces the legacy ``core.settings`` and ``core.registry`` modules
with a *single* Pydantic-powered configuration object.  All user-tuneable
options live in :class:`AppConfig` while the (previously separate) parser
registration / discovery utilities are provided as **class-methods** on the
same model - giving users **one obvious way to configure and introspect
behaviour**.

Key features
------------
1. Strongly-typed fields validated by *Pydantic v2*.
2. Automatic directory creation for filesystem paths.
3. Built-in parser registry with dynamic ``register`` decorator.
4. Lazy global accessor :pyfunc:`get_config` returning a singleton instance.

Usage example
-------------
>>> from doc_parser.config import get_config, AppConfig
>>> cfg = get_config(parser_settings={"pdf": {"dpi": 200}})
>>> print(cfg.cache_dir)
cache/
>>> AppConfig.list_parsers()
{'pdf': ['.pdf'], 'docx': ['.docx'], ...}
"""

from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING, Any, ClassVar

from pydantic import BaseModel, Field, field_validator

from doc_parser.core.exceptions import UnsupportedFormatError

if TYPE_CHECKING:  # pragma: no cover - avoid runtime import cycles
    from collections.abc import Callable

    from doc_parser.core.base import BaseParser

# ---------------------------------------------------------------------------
# AppConfig model
# ---------------------------------------------------------------------------


class AppConfig(BaseModel):
    """Central configuration object **and** parser registry.

    All formerly ad-hoc settings are expressed as strongly-typed fields.  The
    class additionally stores two *class-level* registries mapping parser names
    and file-extensions to their implementing classes.
    """

    # ------------------------------------------------------------------
    # General settings
    # ------------------------------------------------------------------
    cache_dir: Path = Field(default_factory=lambda: Path("cache"))
    output_dir: Path = Field(default_factory=lambda: Path("outputs"))

    max_workers: int = 15
    timeout: int = 60  # seconds
    retry_count: int = 3
    batch_size: int = 1
    use_cache: bool = True

    # ------------------------------------------------------------------
    # Model / LLM settings
    # ------------------------------------------------------------------
    model_provider: str = "openai"
    model_name: str = "gpt-4o-mini"

    # ------------------------------------------------------------------
    # Output / format settings
    # ------------------------------------------------------------------
    output_format: str = "markdown"

    # ------------------------------------------------------------------
    # Parser-specific overrides & post-processing
    # ------------------------------------------------------------------
    parser_settings: dict[str, dict[str, Any]] = Field(default_factory=dict)

    post_prompt: str | None = None
    response_model: str | None = None  # dotted import path to Pydantic model

    # ------------------------------------------------------------------
    # Internal - class-wide parser registry (shared by all instances)
    # ------------------------------------------------------------------
    _parsers: ClassVar[dict[str, type[BaseParser]]] = {}
    _extensions: ClassVar[dict[str, str]] = {}

    # ------------------------------------------------------------------
    # Validators
    # ------------------------------------------------------------------
    @field_validator("cache_dir", "output_dir", mode="before")
    @classmethod
    def _coerce_path_and_mkdir(cls, value: str | Path) -> Path:
        """Ensure the given *value* is a *Path* and create the directory."""
        path = Path(value)
        path.mkdir(parents=True, exist_ok=True)
        return path

    # ------------------------------------------------------------------
    # Convenience helpers
    # ------------------------------------------------------------------
    def parser_cfg(self, name: str) -> dict[str, Any]:
        """Return the configuration dictionary for *parser* *name*."""
        return self.parser_settings.get(name, {})

    # ------------------------------------------------------------------
    # Parser-registry methods (class-level) - mirror old ``AppConfig`` API
    # ------------------------------------------------------------------
    @classmethod
    def register(cls, name: str, extensions: list[str]) -> Callable[[type[BaseParser]], type[BaseParser]]:
        """Decorator to register a *parser* for given *extensions*.

        Example:
        -------
        >>> from doc_parser.config import AppConfig
        >>> @AppConfig.register("txt", [".txt"])
        ... class TxtParser(BaseParser): ...
        """

        def decorator(parser_cls: type[BaseParser]) -> type[BaseParser]:
            # Runtime import to avoid circular dependency at import-time
            from doc_parser.core.base import BaseParser  # local import

            if not issubclass(parser_cls, BaseParser):
                raise TypeError(f"{parser_cls} must inherit from BaseParser")

            if name in cls._parsers:
                raise ValueError(f"Parser '{name}' already registered")

            cls._parsers[name] = parser_cls

            for raw_ext in extensions:
                ext = raw_ext.lower()
                if not ext.startswith("."):
                    ext = f".{ext}"
                cls._extensions[ext] = name

            return parser_cls

        return decorator

    # ------------------------------------------------------------------
    # Parser lookup helpers (factory methods)
    # ------------------------------------------------------------------
    @classmethod
    def from_path(cls, file_path: str | Path, config: AppConfig | None = None) -> BaseParser:
        """Instantiate the appropriate parser for *file_path* (by extension/URL)."""
        from pathlib import Path as _Path  # local import to cut cycles

        # Handle raw strings that might be URLs first
        if isinstance(file_path, str):
            if file_path.startswith(("http://", "https://")):
                parser_name = "html"
                parser_cls = cls._parsers.get(parser_name)
                if parser_cls is None:
                    raise UnsupportedFormatError("HTML parser not registered - cannot handle URL input.")
                return parser_cls(config or get_config())
            file_path = _Path(file_path)

        # Filesystem path handling
        ext = file_path.suffix.lower()
        if ext not in cls._extensions:
            raise UnsupportedFormatError(
                f"No parser found for extension '{ext}'. Supported: {list(cls._extensions.keys())}"
            )

        parser_name = cls._extensions[ext]
        parser_cls = cls._parsers[parser_name]
        return parser_cls(config or get_config())

    @classmethod
    def get_parser_by_name(cls, name: str, config: AppConfig | None = None) -> BaseParser:
        """Instantiate registered parser by *name*."""
        if name not in cls._parsers:
            raise ValueError(f"Parser '{name}' not registered. Available: {list(cls._parsers.keys())}")
        return cls._parsers[name](config or get_config())

    @classmethod
    def list_parsers(cls) -> dict[str, list[str]]:
        """Return mapping of parser names → supported extensions."""
        result: dict[str, list[str]] = {}
        for ext, parser_name in cls._extensions.items():
            result.setdefault(parser_name, []).append(ext)
        return result

    @classmethod
    def is_supported(cls, file_path: Path) -> bool:
        """Return *True* if *file_path* (or extension) is handled by a registered parser."""
        return Path(file_path).suffix.lower() in cls._extensions

    # ------------------------------------------------------------------
    # Pydantic model config
    # ------------------------------------------------------------------
    model_config = {
        "populate_by_name": True,
        "arbitrary_types_allowed": True,
    }


# ---------------------------------------------------------------------------
# Global singleton helpers
# ---------------------------------------------------------------------------

_config_instance: AppConfig | None = None


def get_config(**overrides: Any) -> AppConfig:
    """Return singleton :class:`AppConfig` instance (creating it on first call).

    Optional keyword arguments override default values **only on the initial
    creation**; subsequent calls ignore *overrides* unless the singleton is
    reset manually (e.g., in test-suites).
    """
    global _config_instance  # noqa: PLW0603 - intentional module-level singleton
    if _config_instance is None:
        _config_instance = AppConfig(**overrides)
    return _config_instance



================================================================================
File: doc_parser/core/__init__.py
================================================================================

"""Core functionality for the document parser library."""

from .base import BaseExtractor, BaseParser, ParseResult
from .exceptions import ConfigurationError, ExtractionError, ParserError

__all__ = [
    "BaseExtractor",
    "BaseParser",
    "ConfigurationError",
    "ExtractionError",
    "ParseResult",
    "ParserError",
]



================================================================================
File: doc_parser/core/base.py
================================================================================

"""Abstract base classes for document parsers.

This module provides core abstractions and utilities for building document parsers:

- ``ParseResult``: Data model for parser outputs, including serialization, saving, and metadata handling.
- ``BaseParser``: Abstract base class that handles caching, orchestrates parsing logic, and supports post-processing.
- ``BaseExtractor``: Abstract base class for implementing custom content extractors (e.g., vision or OCR models).

Examples:
>>> from pathlib import Path
>>> from doc_parser.core.base import BaseParser, ParseResult
>>> from doc_parser.core.settings import Settings
>>> settings = Settings(use_cache=False)
>>> class DummyParser(BaseParser):
...     async def _parse(self, input_path: Path, **kwargs):
...         return ParseResult(content="dummy content", metadata={"file": input_path.name})
...
...     async def validate_input(self, input_path: Path) -> bool:
...         return True
>>> import asyncio
>>> parser = DummyParser(settings)
>>> result = asyncio.run(parser.parse(Path("dummy.txt")))
>>> assert isinstance(result, ParseResult)
>>> print(result.content)
"dummy content"
"""

from __future__ import annotations

from abc import ABC, abstractmethod
from datetime import datetime
import hashlib
import json
from typing import TYPE_CHECKING, Any

from pydantic import BaseModel, Field

from doc_parser.utils.cache import cache_get, cache_set

if TYPE_CHECKING:
    from pathlib import Path

    from doc_parser.config import AppConfig
    from doc_parser.prompts import PromptTemplate
    from doc_parser.utils.cache import CacheManager


class ParseResult(BaseModel):
    """Structured result returned by every parser.

    The model provides convenient helpers for serialization and saving.  It is
    intentionally lightweight so that individual parsers can attach arbitrary
    extra fields via the *metadata* dictionary without subclassing.
    """

    content: str = ""
    metadata: dict[str, Any] = Field(default_factory=dict)
    output_format: str = Field(default="markdown", alias="format")
    errors: list[str] = Field(default_factory=list)

    # Post-processing
    post_content: Any | None = None

    # ------------------------------------------------------------------
    # Model configuration
    # ------------------------------------------------------------------
    model_config = {
        "populate_by_name": True,
        "arbitrary_types_allowed": True,
    }

    # ------------------------------------------------------------------
    # Convenience helpers
    # ------------------------------------------------------------------
    def to_dict(self) -> dict[str, Any]:
        """Return ``dict`` representation using *model_dump* with aliases."""
        return self.model_dump(mode="python")

    def to_json(self, **kwargs: Any) -> str:
        """Return JSON string representation.

        Additional keyword arguments are forwarded to :pyfunc:`json.dumps`.
        """
        return json.dumps(self.to_dict(), indent=2, default=str, **kwargs)

    def save_markdown(self, output_path: str | Path) -> None:
        """Write *content* to *output_path* if ``format`` is markdown."""
        from pathlib import Path as _Path

        if self.output_format != "markdown":
            raise ValueError("save_markdown() called on non-markdown result")
        output_path = _Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(self.content, encoding="utf-8")


class BaseParser(ABC):
    """Abstract base class for all parsers."""

    def __init__(self, settings: AppConfig) -> None:
        """Initialize BaseParser with specified settings."""
        # Public so downstream code can tweak if needed
        self.settings: AppConfig = settings

        # Lazily initialised cache manager - created on first access
        self._cache_manager: CacheManager | None = None

    @property
    def cache(self) -> CacheManager:
        """Return lazily initialised cache manager bound to *settings.cache_dir*."""
        if self._cache_manager is None:
            from doc_parser.utils.cache import CacheManager

            self._cache_manager = CacheManager(self.settings.cache_dir)
        return self._cache_manager

    # ------------------------------------------------------------------
    # Public high-level entry-point (caching baked-in)
    # ------------------------------------------------------------------
    async def parse(
        self,
        input_path: Path,
        *,
        output_format: str | None = None,
        options: BaseModel | None = None,
    ) -> ParseResult:
        """Parse *input_path* returning a :class:`ParseResult`.

        Args:
            input_path: Document path or URL to parse.
            output_format: Desired output format ("markdown" or "json"). If
                *None*, the value from :pyattr:`settings.output_format` is used.
            options: Additional parser-specific options

        The method transparently handles caching and post-processing, while
        delegating the heavy-lifting to the subtype-implemented
        :pyfunc:`_parse`. Providing *output_format* avoids mutating
        ``settings.output_format`` at call-sites while maintaining backwards
        compatibility for existing parsers that still reference the setting
        during parsing.
        """
        # ------------------------------------------------------------------
        # Temporarily override settings.output_format (if provided)
        # ------------------------------------------------------------------
        original_format = self.settings.output_format
        if output_format is not None:
            self.settings.output_format = output_format

        try:
            cache_key = self.generate_cache_key(input_path, options=options)

            if self.settings.use_cache:
                cached_result = await cache_get(self.cache, cache_key)
                if cached_result:
                    result = ParseResult(**cached_result)
                    if self.settings.post_prompt and result.post_content is None:
                        await self._run_post_processing(result)
                        await cache_set(self.cache, cache_key, result.to_dict())
                    return result

            # Delegate to concrete parser implementation
            result = await self._parse(input_path, options=options)

            # Post-proc if requested
            if self.settings.post_prompt:
                await self._run_post_processing(result)

            # Persist to cache
            if self.settings.use_cache:
                await cache_set(self.cache, cache_key, result.to_dict())

            return result
        finally:
            # Always restore original output_format to avoid side-effects
            self.settings.output_format = original_format

    # ------------------------------------------------------------------
    # Default structured parsing logic (validate → open → extract)
    # ------------------------------------------------------------------
    async def _parse(self, input_path: Path, *, options: BaseModel | None = None) -> ParseResult:
        """Default structured-document parsing implementation.

        Subclasses can override for custom behaviour, or simply implement the helper hooks:

        * validate_input
        * _open_document
        * _extract_as_markdown
        * _extract_as_json
        * _extra_metadata (optional)
        """
        # 1) Validate the input first
        if not await self.validate_input(input_path):
            return ParseResult(
                content="",
                metadata=self.get_metadata(input_path),
                errors=[f"Invalid file: {input_path}"],
            )

        try:
            # 2) Open the document using the subclass hook
            document_obj = await self._open_document(input_path, options=options)

            # 3) Extract content according to requested format
            if self.settings.output_format == "markdown":
                content = await self._extract_as_markdown(document_obj)
            elif self.settings.output_format == "json":
                content = await self._extract_as_json(document_obj)
            else:
                # Fallback to markdown for unsupported formats
                content = await self._extract_as_markdown(document_obj)

            # 4) Aggregate metadata
            metadata = self.get_metadata(input_path)
            metadata.update(self._extra_metadata(document_obj))

            return ParseResult(content=content, metadata=metadata, output_format=self.settings.output_format)

        except Exception as exc:  # pylint: disable=broad-except  # noqa: BLE001
            # Gracefully convert unexpected exception into a ParseResult so callers don't crash
            return ParseResult(
                content="",
                metadata=self.get_metadata(input_path),
                errors=[f"Failed to parse {input_path.name}: {exc!s}"],
            )

    # ------------------------------------------------------------------
    # Hook methods for structured parsers
    # ------------------------------------------------------------------
    async def _open_document(self, input_path: Path, *, options: BaseModel | None = None) -> Any:
        """Open *input_path* and return a third-party document object.

        Structured parsers **must** override this. The base implementation just raises.
        """
        raise NotImplementedError

    async def _extract_as_markdown(self, document_obj: Any) -> str:
        """Convert *document_obj* to Markdown. Must be overridden by structured parsers."""
        raise NotImplementedError

    async def _extract_as_json(self, document_obj: Any) -> str:
        """Serialize *document_obj* to a JSON string. Must be overridden by structured parsers."""
        raise NotImplementedError

    def _extra_metadata(self, _document_obj: Any) -> dict[str, Any]:
        """Return extra metadata dict; subclasses may override."""
        return {}

    @abstractmethod
    async def validate_input(self, input_path: Path) -> bool:
        """Validate if the input file can be parsed.

        Args:
            input_path: Path to the input file

        Returns:
            True if file can be parsed, False otherwise
        """

    def generate_cache_key(self, input_path: Path, *, options: BaseModel | None = None) -> str:
        """Return a stable cache key for *input_path* and current parser settings."""
        key_data = {
            "file": str(input_path.absolute()),
            "mtime": input_path.stat().st_mtime,
            "settings": self.settings.model_dump(),
            "options": options.model_dump() if isinstance(options, BaseModel) else None,
        }
        key_str = json.dumps(key_data, sort_keys=True, default=str)
        return hashlib.sha256(key_str.encode()).hexdigest()

    def get_metadata(self, input_path: Path) -> dict[str, Any]:
        """Get basic file metadata."""
        stat = input_path.stat()
        return {
            "filename": input_path.name,
            "size": stat.st_size,
            "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "parser": self.__class__.__name__,
        }

    async def _run_post_processing(self, result: ParseResult) -> None:
        """Helper to perform post-processing and attach to *result*."""
        import logging

        logger = logging.getLogger(__name__)
        try:
            from doc_parser.utils.llm_post_processor import LLMPostProcessor

            logger.debug("Starting post-processing with prompt=%s", self.settings.post_prompt)
            post_processor = LLMPostProcessor(self.settings, cache_manager=self.cache)
            prompt_arg: str = self.settings.post_prompt or ""
            result.post_content = await post_processor.process(result.content, prompt_arg)
            logger.debug("Post-processing complete. Length=%s", len(str(result.post_content)))
        except Exception:  # pylint: disable=broad-except
            logger.exception("Post-processing failed")
            result.errors.append("Post-processing failed")

    # ------------------------------------------------------------------
    # Public convenience property for accessing settings via .config
    # ------------------------------------------------------------------
    @property
    def config(self) -> AppConfig:
        """Alias property for settings (preferred)."""
        return self.settings

    # ------------------------------------------------------------------
    # Convenience wrappers for explicit output modes
    # ------------------------------------------------------------------

    async def parse_markdown(self, input_path: Path, *, options: BaseModel | None = None) -> ParseResult:
        """Parse *input_path* and return Markdown content.

        A thin wrapper around :pyfunc:`parse` that sets ``output_format`` to
        ``"markdown"`` without mutating global settings.
        """
        return await self.parse(input_path, output_format="markdown", options=options)

    async def parse_json(self, input_path: Path, *, options: BaseModel | None = None) -> ParseResult:
        """Parse *input_path* and return JSON content (as string)."""
        return await self.parse(input_path, output_format="json", options=options)

    # ------------------------------------------------------------------
    # Class-level helpers
    # ------------------------------------------------------------------

    @classmethod
    def supported_extensions(cls) -> list[str]:
        """Return the list of file extensions associated with *cls*.

        The information is retrieved from the global :class:`AppConfig` parser
        registry so that callers no longer rely on the legacy
        ``SUPPORTED_EXTENSIONS`` class attribute.
        """
        # Runtime import to avoid import-time cycles
        from doc_parser.config import AppConfig  # local import

        # Identify all *parser names* that map to this class
        parser_names = [name for name, p_cls in AppConfig._parsers.items() if p_cls is cls]

        # Collect extensions whose registered parser matches any of the names
        return [ext for ext, name in AppConfig._extensions.items() if name in parser_names]

    # ------------------------------------------------------------------
    # Path helpers (formerly utils.file_validators)
    # ------------------------------------------------------------------
    def _has_supported_extension(self, input_path: Path) -> bool:
        """Return ``True`` if *input_path* matches this parser's extensions.

        Args:
            input_path: Path to validate.

        Returns:
            bool: ``True`` when the suffix of *input_path* (case-insensitive) is
                one of :pyattr:`SUPPORTED_EXTENSIONS`.
        """
        ext = input_path.suffix.lower()
        return ext in self.supported_extensions()


class BaseExtractor(ABC):
    """Base class for content extractors."""

    @abstractmethod
    async def extract(self, content: Any, prompt_template: PromptTemplate | None = None) -> str:
        """Extract structured information from content.

        Args:
            content: Content to extract from (format depends on implementation)
            prompt_template: Optional custom prompt template

        Returns:
            Extracted text content
        """

    @abstractmethod
    def get_default_prompt(self) -> str:
        """Get the default prompt for this extractor."""


# ------------------------------------------------------------------
# Explicit re-export list for "from doc_parser.core.base import *" usage
# ------------------------------------------------------------------
__all__ = [
    "BaseExtractor",
    "BaseParser",
    "ParseResult",
]



================================================================================
File: doc_parser/core/exceptions.py
================================================================================

"""Custom exceptions for the document parser library.

This module defines exception types used across the doc_parser library
to signal various error conditions during parsing, configuration,
extraction, and caching.

Examples:
>>> from doc_parser.core.exceptions import ParserError, ConfigurationError
>>> try:
...     raise ConfigurationError("Invalid value for 'dpi'")
... except ParserError as e:
...     print(f"Caught error: {e}")
Caught error: Invalid value for 'dpi'
"""


class ParserError(Exception):
    """Base exception for all parser-related errors.

    All custom exceptions in this library inherit from ParserError.
    """


class ConfigurationError(ParserError):
    """Raised when there's an issue with parser or application configuration.

    This error indicates invalid or missing settings that prevent parsing.

    Examples:
    >>> from doc_parser.core.exceptions import ConfigurationError
    >>> raise ConfigurationError("Missing required setting: 'cache_dir'")
    Traceback (most recent call last):
      ...
    ConfigurationError: Missing required setting: 'cache_dir'
    """


class ExtractionError(ParserError):
    """Raised when content extraction fails.

    Signals errors during content extraction (e.g., OCR failures).

    Examples:
    >>> from doc_parser.core.exceptions import ExtractionError
    >>> raise ExtractionError("Failed to extract text from image")
    Traceback (most recent call last):
      ...
    ExtractionError: Failed to extract text from image
    """


class UnsupportedFormatError(ParserError):
    """Raised when attempting to parse an unsupported file format.

    Indicates no parser is registered for the given extension or input.

    Examples:
    >>> from doc_parser.core.exceptions import UnsupportedFormatError
    >>> raise UnsupportedFormatError(".txt files are not supported")
    Traceback (most recent call last):
      ...
    UnsupportedFormatError: .txt files are not supported
    """


class CacheError(ParserError):
    """Raised when cache operations fail.

    Includes read/write errors or TTL expiration issues in the cache.

    Examples:
    >>> from doc_parser.core.exceptions import CacheError
    >>> raise CacheError("Unable to write to cache directory")
    Traceback (most recent call last):
      ...
    CacheError: Unable to write to cache directory
    """


# ------------------------------------------------------------------
# Public exports
# ------------------------------------------------------------------
__all__ = [
    "CacheError",
    "ConfigurationError",
    "ExtractionError",
    "ParserError",
    "UnsupportedFormatError",
]



================================================================================
File: doc_parser/core/types.py
================================================================================

"""Common type aliases used across the library.

Type Aliases:
    PathLike: Union[str, Path]
        Alias for filesystem path parameters, accepts both string and Path.

Examples:
    >>> from doc_parser.core.types import PathLike
    >>> def read(path: PathLike) -> str:
    ...     from pathlib import Path
    ...
    ...     return Path(path).read_text()
    >>> read("example.txt")
    "Hello"
"""

from pathlib import Path

type PathLike = str | Path

"""Alias for path-like inputs (str or Path).

This alias is used throughout the library for parameters representing
filesystem paths to allow flexible input types.

Examples:
>>> from doc_parser.core.types import PathLike
>>> def save(content: str, output: PathLike) -> None:
...     from pathlib import Path
...     Path(output).write_text(content)
"""

# ------------------------------------------------------------------
# Public exports
# ------------------------------------------------------------------
__all__ = [
    "PathLike",
]



================================================================================
File: doc_parser/options.py
================================================================================

"""Option objects for parser runtime configuration.

These classes replace the previous ad-hoc ``**kwargs`` mechanism that passed
miscellaneous options into ``BaseParser.parse``.  Each parser now receives a
strongly-typed options object, improving validation and static type-checking.
"""

from __future__ import annotations

from pydantic import BaseModel, Field, validator  # Pydantic v1 compatible

# Standard library

# ----------------------------------------------------------------------------
# Shared helpers
# ----------------------------------------------------------------------------

PageRange = tuple[int, int]


class _BaseOptions(BaseModel):  # pylint: disable=too-few-public-methods
    """Common ancestor to allow isinstance checks across option objects."""

    class Config:  # pylint: disable=too-few-public-methods
        arbitrary_types_allowed = True
        allow_population_by_field_name = True
        validate_assignment = True
        extra = "forbid"  # Disallow unexpected kwargs to catch typos early.


# ----------------------------------------------------------------------------
# Parser-specific option models
# ----------------------------------------------------------------------------


class PdfOptions(_BaseOptions):
    """Run-time options specific to the PDF parser."""

    page_range: PageRange | None = Field(
        default=None,
        description="Inclusive 1-based (start, end) page range to extract.  If\n        *None*, the full document is processed.",
    )
    prompt_template: str | None = Field(
        default=None,
        description="Custom prompt template (template string or ID) used during\n        downstream LLM summarisation, if applicable.",
    )

    # Ensure correct ordering of the tuple and positive indices
    @validator("page_range")
    def _validate_page_range(cls, v: PageRange | None) -> PageRange | None:  # noqa: N805
        if v is None:
            return v
        start, end = v
        if start <= 0 or end <= 0:
            raise ValueError("page_range values must be positive and 1-based")
        if start > end:
            raise ValueError("page_range start must be <= end")
        return v


class HtmlOptions(_BaseOptions):
    """Run-time options specific to the HTML parser."""

    extract_sources: bool | None = Field(
        default=None,
        description="Whether to include source links embedded in the page content.",
    )
    follow_links: bool | None = Field(
        default=None,
        description="If *True*, the parser will crawl linked pages up to\n        *max_depth*.",
    )
    max_depth: int | None = Field(
        default=None,
        ge=1,
        description="Maximum recursion depth when *follow_links* is *True*.",
    )


class DocxOptions(_BaseOptions):
    """Run-time options specific to the DOCX parser."""

    extract_images: bool | None = Field(default=None)
    extract_headers_footers: bool | None = Field(default=None)
    preserve_formatting: bool | None = Field(default=None)


class ExcelOptions(_BaseOptions):
    """Run-time options specific to the Excel parser."""

    include_formulas: bool | None = Field(default=None)
    include_formatting: bool | None = Field(default=None)
    sheet_names: list[str] | None = Field(
        default=None,
        description="Subset of sheet names to parse; *None* parses all sheets.",
    )


class PptxOptions(_BaseOptions):
    """Run-time options specific to the PPTX parser."""

    extract_images: bool | None = Field(default=None)
    extract_notes: bool | None = Field(default=None)
    preserve_formatting: bool | None = Field(default=None)
    slide_delimiter: str | None = Field(default=None)


# ----------------------------------------------------------------------------
# Public re-exports
# ----------------------------------------------------------------------------

__all__ = [
    "DocxOptions",
    "ExcelOptions",
    "HtmlOptions",
    "PageRange",
    "PdfOptions",
    "PptxOptions",
]



================================================================================
File: doc_parser/parsers/__init__.py
================================================================================

"""Document parser implementations."""

# Import parsers to trigger registration
from . import docx, excel, html, pdf, pptx

__all__ = ["docx", "excel", "html", "pdf", "pptx"]



================================================================================
File: doc_parser/parsers/docx/__init__.py
================================================================================

"""DOCX parser implementation."""

from .parser import DocxParser

__all__ = ["DocxParser"]



================================================================================
File: doc_parser/parsers/docx/parser.py
================================================================================

"""DOCX parser implementation.

This module provides a parser for Microsoft Word .docx files with:
- Content extraction as Markdown or JSON
- Image and header/footer extraction
- Metadata extraction (paragraph, table, section counts, author, title)

Examples:
>>> from pathlib import Path
>>> import asyncio
>>> from doc_parser.parsers.docx.parser import DocxParser
>>> from doc_parser.core.settings import Settings
>>> settings = Settings(output_format="markdown")
>>> parser = DocxParser(settings)
>>> result = asyncio.run(parser.parse(Path("example.docx")))
>>> print(result.metadata["paragraphs"])  # Number of paragraphs extracted
"""

from collections.abc import Iterable
import json
from pathlib import Path
from typing import TYPE_CHECKING, Any

import docx
from docx.document import Document
from docx.opc.exceptions import PackageNotFoundError
from docx.table import Table
from docx.text.paragraph import Paragraph

from doc_parser.config import AppConfig
from doc_parser.core.base import BaseParser
from doc_parser.utils.format_helpers import rows_to_markdown

if TYPE_CHECKING:  # pragma: no cover
    from pydantic import BaseModel


@AppConfig.register("docx", [".docx"])
class DocxParser(BaseParser):
    """Parser for Microsoft Word documents (.docx).

    Provides methods to validate, parse, and extract content in Markdown or JSON formats.

    Args:
        config (Settings): Global parser settings instance.

    Attributes:
        extract_images (bool): Whether to extract embedded images.
        extract_headers_footers (bool): Whether to include headers and footers.
        preserve_formatting (bool): Whether to preserve bold, italic, and underline.

    Examples:
        >>> from pathlib import Path
        >>> import asyncio
        >>> from doc_parser.parsers.docx.parser import DocxParser
        >>> from doc_parser.core.settings import Settings
        >>> settings = Settings(parser_settings={"docx": {"extract_images": False}})
        >>> parser = DocxParser(settings)
        >>> result = asyncio.run(parser.parse(Path("sample.docx")))
        >>> assert "tables" in result.metadata
    """

    def __init__(self, config: AppConfig):
        """Initialize DOCX parser."""
        super().__init__(config)

        # Get DOCX-specific settings
        docx_config = config.parser_cfg("docx")
        self.extract_images = docx_config.get("extract_images", True)
        self.extract_headers_footers = docx_config.get("extract_headers_footers", False)
        self.preserve_formatting = docx_config.get("preserve_formatting", True)

    async def validate_input(self, input_path: Path) -> bool:
        """Validate whether the input path points to a valid DOCX file.

        Args:
            input_path (Path): Path to the .docx file.

        Returns:
            bool: True if the file is a valid DOCX document, False otherwise.

        Example:
            >>> import asyncio
            >>> from pathlib import Path
            >>> valid = asyncio.run(DocxParser(Settings()).validate_input(Path("doc.docx")))
            >>> print(valid)
        """
        if not self._has_supported_extension(input_path):
            return False
        try:
            # Try to open with python-docx to validate
            docx.Document(str(input_path))
        except PackageNotFoundError:
            return False
        return True

    # ------------------------------------------------------------------
    # BaseStructuredParser hooks
    # ------------------------------------------------------------------

    async def _open_document(self, input_path: Path, *, options: "BaseModel | None" = None) -> Document:
        """Open *input_path* with **python-docx** and return a Document object.

        The *options* parameter is accepted for API compatibility but is not
        used by the DOCX parser at this time.
        """
        _ = options  # future-proof - avoid unused-arg warnings
        return docx.Document(str(input_path))

    def _extra_metadata(self, doc: Any) -> dict[str, Any]:
        """Return DOCX-specific metadata counts and core properties."""
        # Cast to python-docx Document for type-check; if wrong type we fall back gracefully
        from docx.document import Document

        if isinstance(doc, Document):
            document = doc
        else:
            return {}

        meta: dict[str, Any] = {
            "paragraphs": len(document.paragraphs),
            "tables": len(document.tables),
            "sections": len(document.sections),
        }

        # Optional core properties
        if getattr(document.core_properties, "title", None):
            meta["title"] = document.core_properties.title
        if getattr(document.core_properties, "author", None):
            meta["author"] = document.core_properties.author
        return meta

    async def _extract_as_markdown(self, doc: Document) -> str:
        """Convert a python-docx Document to a Markdown string.

        Iterates through paragraphs and tables, generating GitHub-flavored Markdown.
        Includes headers and footers if enabled in settings.

        Args:
            doc (Document): python-docx Document object.

        Returns:
            str: Combined Markdown content.

        Example:
            >>> import asyncio, docx
            >>> from pathlib import Path
            >>> parser = DocxParser(Settings())
            >>> doc = docx.Document(str(Path("doc.docx")))
            >>> md = asyncio.run(parser._extract_as_markdown(doc))
            >>> assert "|" in md or md.startswith("#")
        """
        content_parts: list[str] = []

        # Process document elements in order
        for element in self._iter_block_items(doc):
            if isinstance(element, Paragraph):
                md_text = self._paragraph_to_markdown(element)
                if md_text.strip():
                    content_parts.append(md_text)
            elif isinstance(element, Table):
                md_table = self._table_to_markdown(element)
                if md_table:
                    content_parts.append(md_table)

        # Extract headers/footers if requested
        if self.extract_headers_footers:
            headers_footers = self._extract_headers_footers(doc)
            if headers_footers:
                content_parts.append("\n---\n## Headers and Footers\n")
                content_parts.append(headers_footers)

        return "\n\n".join(content_parts)

    async def _extract_as_json(self, doc: Document) -> str:
        """Serialize DOCX content into a JSON string.

        Gathers paragraphs and tables into JSON arrays, with document properties.

        Args:
            doc (Document): python-docx Document object.

        Returns:
            str: JSON-formatted content with keys 'paragraphs', 'tables', and 'properties'.

        Example:
            >>> import asyncio, json, docx
            >>> from pathlib import Path
            >>> parser = DocxParser(Settings())
            >>> doc = docx.Document(str(Path("doc.docx")))
            >>> js = asyncio.run(parser._extract_as_json(doc))
            >>> data = json.loads(js)
            >>> assert isinstance(data.get("paragraphs"), list)
        """
        data: dict[str, Any] = {
            "paragraphs": [],
            "tables": [],
            "properties": {},
        }

        # Extract paragraphs
        for para in doc.paragraphs:
            if para.text.strip():
                para_data = {
                    "text": para.text,
                    "style": para.style.name if para.style else None,
                }
                if self.preserve_formatting:
                    para_data["runs"] = [
                        {
                            "text": run.text,
                            "bold": run.bold,
                            "italic": run.italic,
                            "underline": run.underline,
                        }
                        for run in para.runs
                    ]
                paragraphs = data["paragraphs"]
                if isinstance(paragraphs, list):
                    paragraphs.append(para_data)

        # Extract tables
        for table in doc.tables:
            # Use list comprehension for performance
            table_data = [[cell.text.strip() for cell in row.cells] for row in table.rows]
            tables_list = data["tables"]
            if isinstance(tables_list, list):
                tables_list.append(table_data)

        # Add properties
        if hasattr(doc.core_properties, "title"):
            data["properties"]["title"] = doc.core_properties.title
        if hasattr(doc.core_properties, "author"):
            data["properties"]["author"] = doc.core_properties.author

        return json.dumps(data, indent=2, ensure_ascii=False)

    def _iter_block_items(self, parent: Document | Any) -> Iterable[Paragraph | Table]:
        """Yield each paragraph and table child within parent, in document order."""
        from docx.oxml.table import CT_Tbl
        from docx.oxml.text.paragraph import CT_P
        from docx.table import Table, _Cell
        from docx.text.paragraph import Paragraph

        if isinstance(parent, Document):
            parent_elm = parent.element.body
        elif isinstance(parent, _Cell):
            parent_elm = parent._tc
        else:
            raise TypeError("Parent must be Document or _Cell")

        for child in parent_elm.iterchildren():
            if isinstance(child, CT_P):
                yield Paragraph(child, parent)
            elif isinstance(child, CT_Tbl):
                yield Table(child, parent)

    def _paragraph_to_markdown(self, paragraph: Paragraph) -> str:
        """Convert a python-docx Paragraph to Markdown with reduced branching."""
        text = paragraph.text.strip()
        if not text:
            return ""

        style_name = paragraph.style.name if paragraph.style else ""

        # -------------------------------------------------------------
        # Heading styles 1-6
        # -------------------------------------------------------------
        min_heading_prefix_len = 9  # len('Heading X')
        if style_name.startswith("Heading ") and len(style_name) >= min_heading_prefix_len and style_name[8].isdigit():
            level_char = style_name[8]
            level = int(level_char)
            level = max(1, min(level, 6))
            return "#" * level + f" {text}"

        # -------------------------------------------------------------
        # List items
        # -------------------------------------------------------------
        if style_name.startswith("List"):
            bullet_prefix = "- " if "Bullet" in style_name else "1. "
            return f"{bullet_prefix}{text}"

        # -------------------------------------------------------------
        # Inline formatting (bold / italic / underline)
        # -------------------------------------------------------------
        if self.preserve_formatting and paragraph.runs:
            pieces: list[str] = []
            for run in paragraph.runs:
                run_text = run.text
                if run.bold:
                    run_text = f"**{run_text}**"
                if run.italic:
                    run_text = f"*{run_text}*"
                if run.underline:
                    run_text = f"<u>{run_text}</u>"
                pieces.append(run_text)
            text = "".join(pieces) if pieces else text

        return text

    def _table_to_markdown(self, table: Table) -> str:
        """Convert a table to Markdown."""
        rows = [[cell.text.strip() for cell in row.cells] for row in table.rows]
        return rows_to_markdown(rows)

    def _extract_headers_footers(self, doc: Document) -> str:
        """Extract headers and footers from document."""
        content = []

        for section in doc.sections:
            # Header
            if section.header and section.header.paragraphs:
                header_text = " ".join(p.text.strip() for p in section.header.paragraphs if p.text.strip())
                if header_text:
                    content.append(f"**Header:** {header_text}")

            # Footer
            if section.footer and section.footer.paragraphs:
                footer_text = " ".join(p.text.strip() for p in section.footer.paragraphs if p.text.strip())
                if footer_text:
                    content.append(f"**Footer:** {footer_text}")

        return "\n".join(content)

    def _has_supported_extension(self, input_path: Path) -> bool:
        return super()._has_supported_extension(input_path)



================================================================================
File: doc_parser/parsers/excel/__init__.py
================================================================================

"""Excel parser implementation."""

from .parser import ExcelParser

__all__ = ["ExcelParser"]



================================================================================
File: doc_parser/parsers/excel/parser.py
================================================================================

"""Excel parser implementation.

This module provides a parser for Microsoft Excel files (.xlsx, .xls, .xlsm). It
supports extraction of sheet data into Markdown or JSON, optional formulas and
formatting, and metadata aggregation (sheet names, counts).

Examples:
>>> from pathlib import Path
>>> import asyncio
>>> from doc_parser.parsers.excel.parser import ExcelParser
>>> from doc_parser.core.settings import Settings
>>> settings = Settings(output_format="json", parser_settings={"excel": {"include_formulas": True}})
>>> parser = ExcelParser(settings)
>>> result = asyncio.run(parser.parse(Path("example.xlsx")))
>>> assert "sheets" in result.metadata
"""

from pathlib import Path
from typing import TYPE_CHECKING, Any

import openpyxl
from openpyxl.utils import get_column_letter
from openpyxl.utils.exceptions import InvalidFileException
import pandas as pd

from doc_parser.config import AppConfig
from doc_parser.core.base import BaseParser
from doc_parser.utils.format_helpers import dataframe_to_markdown

if TYPE_CHECKING:  # pragma: no cover
    from pydantic import BaseModel


@AppConfig.register("excel", [".xlsx", ".xls", ".xlsm"])
class ExcelParser(BaseParser):
    """Parser for Excel files (.xlsx, .xls, .xlsm).

    Args:
        config (Settings): Global settings, with optional Excel-specific keys:
            - include_formulas (bool): Whether to extract cell formulas (default False).
            - include_formatting (bool): Whether to include cell formatting (default False).
            - sheet_names (List[str] | None): Specific sheets to parse; None for all (default).

    Attributes:
        include_formulas (bool): If True, extract formulas.
        include_formatting (bool): If True, include formatting.
        sheet_names (List[str] | None): Sheets to process.

    Examples:
        >>> from pathlib import Path
        >>> import asyncio
        >>> from doc_parser.parsers.excel.parser import ExcelParser
        >>> from doc_parser.core.settings import Settings
        >>> settings = Settings(output_format="markdown", parser_settings={"excel": {"include_formulas": True}})
        >>> parser = ExcelParser(settings)
        >>> result = asyncio.run(parser.parse(Path("data.xlsx")))
        >>> print(result.format)
        'markdown'
    """

    def __init__(self, config: AppConfig):
        """Initialize Excel parser."""
        super().__init__(config)

        # Get Excel-specific settings
        excel_config = config.parser_cfg("excel")
        self.include_formulas = excel_config.get("include_formulas", False)
        self.include_formatting = excel_config.get("include_formatting", False)
        self.sheet_names = excel_config.get("sheet_names", None)  # None means all sheets

    async def validate_input(self, input_path: Path) -> bool:
        """Validate whether the input path points to a readable Excel file.

        Args:
            input_path (Path): Path to the Excel file.

        Returns:
            bool: True if file exists, has supported extension, and is readable via pandas.

        Example:
            >>> import asyncio
            >>> from pathlib import Path
            >>> parser = ExcelParser(Settings())
            >>> valid = asyncio.run(parser.validate_input(Path("file.xlsx")))
            >>> print(valid)
        """
        if not self._has_supported_extension(input_path):
            return False
        try:
            # Try to open with pandas to validate
            pd.ExcelFile(input_path)
        except (InvalidFileException, ValueError, OSError):
            return False
        return True

    # ------------------------------------------------------------------
    # BaseStructuredParser hooks
    # ------------------------------------------------------------------

    async def _open_document(self, input_path: Path, *, options: "BaseModel | None" = None) -> Path:
        """Return *input_path* so downstream helpers can open via pandas as needed."""
        _ = options
        return input_path

    def _extra_metadata(self, input_path: Any) -> dict[str, Any]:
        """Return sheet names and count for quick metadata lookup."""
        try:
            excel_file = pd.ExcelFile(input_path)
            return {
                "sheets": excel_file.sheet_names,
                "sheet_count": len(excel_file.sheet_names),
            }
        except Exception:  # pylint: disable=broad-except  # noqa: BLE001
            return {}

    async def _extract_as_markdown(self, document_obj: Path) -> str:
        """Extract Excel content as a Markdown string.

        Iterates over sheets, converts DataFrames to markdown tables, includes empty sheet markers,
        and appends formulas if configured.

        Args:
            document_obj (Path): Path to the Excel file.

        Returns:
            str: Combined Markdown content for all processed sheets.

        Example:
            >>> import asyncio
            >>> from pathlib import Path
            >>> parser = ExcelParser(Settings(parser_settings={"excel": {"include_formulas": True}}))
            >>> md = asyncio.run(parser._extract_as_markdown(Path("file.xlsx")))
            >>> assert "# Sheet:" in md
        """
        excel_file = pd.ExcelFile(document_obj)
        content_parts = []

        # Determine which sheets to process
        if self.sheet_names is not None:
            sheets_to_process: list[str] = [s for s in self.sheet_names if s in excel_file.sheet_names]
        else:
            sheets_to_process = [str(name) for name in excel_file.sheet_names]

        for sheet_name in sheets_to_process:
            # Add sheet header
            content_parts.append(f"# Sheet: {sheet_name}\n")

            # Read sheet data
            df = pd.read_excel(document_obj, sheet_name=sheet_name, header=None)

            # Convert to markdown table
            if not df.empty:
                markdown_table = self._dataframe_to_markdown(df)
                content_parts.append(markdown_table)
            else:
                content_parts.append("*Empty sheet*")

            # Add formulas if requested
            if self.include_formulas:
                formulas = await self._extract_formulas(document_obj, sheet_name)
                if formulas:
                    content_parts.append("\n## Formulas\n")
                    for cell, formula in formulas.items():
                        content_parts.append(f"- {cell}: `{formula}`")

            content_parts.append("\n")

        return "\n".join(content_parts)

    async def _extract_as_json(self, document_obj: Path) -> str:
        """Extract Excel content as a JSON string.

        Serializes each sheet into an object containing data records, column names, shape,
        and optional formulas.

        Args:
            document_obj (Path): Path to the Excel file.

        Returns:
            str: JSON-formatted string of sheet data.

        Example:
            >>> import asyncio, json
            >>> from pathlib import Path
            >>> parser = ExcelParser(Settings())
            >>> js = asyncio.run(parser._extract_as_json(Path("file.xlsx")))
            >>> data = json.loads(js)
            >>> assert isinstance(data, dict)
        """
        import json

        excel_file = pd.ExcelFile(document_obj)
        data = {}

        # Determine which sheets to process
        if self.sheet_names is not None:
            sheets_to_process: list[str] = [s for s in self.sheet_names if s in excel_file.sheet_names]
        else:
            sheets_to_process = [str(name) for name in excel_file.sheet_names]

        for sheet_name in sheets_to_process:
            df = pd.read_excel(document_obj, sheet_name=sheet_name)

            # Convert to dict
            sheet_data = {
                "data": df.to_dict(orient="records"),
                "columns": df.columns.tolist(),
                "shape": df.shape,
            }

            # Add formulas if requested
            if self.include_formulas:
                sheet_data["formulas"] = await self._extract_formulas(document_obj, sheet_name)

            data[sheet_name] = sheet_data

        return json.dumps(data, indent=2, default=str)

    def _dataframe_to_markdown(self, df: pd.DataFrame) -> str:
        """Convert a pandas DataFrame to a GitHub-flavored Markdown table.

        Args:
            df (pd.DataFrame): DataFrame to convert.

        Returns:
            str: Markdown table as string.

        Example:
            >>> import pandas as pd
            >>> from doc_parser.parsers.excel.parser import ExcelParser
            >>> df = pd.DataFrame([[1, 2], [3, 4]])
            >>> md = ExcelParser(Settings())._dataframe_to_markdown(df)
            >>> assert "|" in md
        """
        return dataframe_to_markdown(df)

    async def _extract_formulas(self, input_path: Path, sheet_name: str) -> dict[str, str]:
        """Extract cell formulas from a specific sheet of an Excel file.

        Args:
            input_path (Path): Path to the Excel file.
            sheet_name (str): Name of the sheet to extract formulas from.

        Returns:
            Dict[str, str]: Mapping of cell references to formula strings.

        Example:
            >>> import asyncio
            >>> from pathlib import Path
            >>> parser = ExcelParser(Settings(parser_settings={"excel": {"include_formulas": True}}))
            >>> formulas = asyncio.run(parser._extract_formulas(Path("file.xlsx"), "Sheet1"))
            >>> assert isinstance(formulas, dict)
        """
        formulas: dict[str, str] = {}
        try:
            wb = openpyxl.load_workbook(str(input_path), data_only=False)
            try:
                ws = wb[sheet_name]
                for row in ws.iter_rows():
                    for cell in row:
                        if cell.value and isinstance(cell.value, str) and cell.value.startswith("="):
                            cell_ref = f"{get_column_letter(cell.column)}{cell.row}"
                            formulas[cell_ref] = cell.value
            finally:
                wb.close()
        except (InvalidFileException, KeyError, OSError):
            # Failed to load workbook or sheet not found
            return {}
        return formulas



================================================================================
File: doc_parser/parsers/html/__init__.py
================================================================================

"""HTML parser implementation."""

from .parser import HtmlParser

__all__ = [
    "HtmlParser",
]



================================================================================
File: doc_parser/parsers/html/parser.py
================================================================================

"""HTML parser implementation for web content and Perplexity.ai exports.

This module provides the HtmlParser, capable of parsing:
- Local HTML files with caching and content-type handling
- .url and .webloc link files
- Remote URLs via HTTP/HTTPS
- Perplexity.ai query/answer pages with specialized extraction

Features:
- Caching to disk
- Configurable source extraction, link following, depth limits
- Output formats: Markdown or JSON

Examples:
>>> import asyncio
>>> from doc_parser.core.settings import Settings
>>> from doc_parser.parsers.html.parser import HtmlParser
>>> settings = Settings(parser_settings={"html": {"extract_sources": True, "follow_links": False}})
>>> parser = HtmlParser(settings)
>>> result = asyncio.run(parser.parse("https://example.com"))
>>> print(result.content[:100])
"""

from __future__ import annotations

import plistlib
import re
from typing import TYPE_CHECKING, Any
from urllib.parse import urlparse

import aiohttp
from bs4 import BeautifulSoup
from bs4.element import Tag
import html2text

from doc_parser.config import AppConfig
from doc_parser.core.base import BaseParser, ParseResult

if TYPE_CHECKING:
    from pathlib import Path

    from pydantic import BaseModel

MIN_RELATED_LEN = 10


@AppConfig.register("html", [".html", ".htm", ".pplx", ".url", ".webloc"])
class HtmlParser(BaseParser):
    """Parser for HTML pages, Perplexity.ai exports, and generic web content.

    This parser handles:
    - Validating and parsing local link files (.url, .webloc)
    - Fetching and parsing HTTP/HTTPS URLs
    - Specialized extraction for Perplexity.ai query/answer pages
    - Configurable extraction of sources and link following
    - Output in Markdown or JSON formats

    Args:
        config (Settings): Global settings with optional 'html' parser config:
            - extract_sources (bool): Include link sources (default True)
            - follow_links (bool): Follow embedded links (default False)
            - max_depth (int): Maximum link-following depth (default 1)

    Examples:
        >>> import asyncio
        >>> from doc_parser.core.settings import Settings
        >>> from doc_parser.parsers.html.parser import HtmlParser
        >>> settings = Settings(parser_settings={"html": {"follow_links": True, "max_depth": 2}})
        >>> parser = HtmlParser(settings)
        >>> result = asyncio.run(parser.parse("example.url"))
        >>> assert "## Links" in result.content
    """

    def __init__(self, config: AppConfig):
        """Initialize HTML parser with configuration."""
        super().__init__(config)

        html_cfg = config.parser_cfg("html")
        self.extract_sources: bool = html_cfg.get("extract_sources", True)
        self.follow_links: bool = html_cfg.get("follow_links", False)
        self.max_depth: int = html_cfg.get("max_depth", 1)

        # HTML→Markdown converter
        self.h2t = html2text.HTML2Text()
        self.h2t.ignore_links = False
        self.h2t.ignore_images = False
        self.h2t.body_width = 0

    # ------------------------------------------------------------------
    # Public high-level entry-point override to support URL strings
    # ------------------------------------------------------------------
    async def parse(
        self, input_path: Path | str, *, output_format: str | None = None, options: BaseModel | None = None
    ) -> ParseResult:
        """Parse input_path as local file or remote URL.

        Overrides BaseParser.parse to route HTTP/HTTPS inputs to parse_url
        and use caching for local files.

        Args:
            input_path (Path | str): File path or URL string.
            output_format (str | None): Output format for the parsed content.
            options (BaseModel | None): Optional parser-specific options.

        Returns:
            ParseResult: Parsed content, metadata, format, and errors.

        Example:
            >>> import asyncio
            >>> from doc_parser.core.settings import Settings
            >>> from doc_parser.parsers.html.parser import HtmlParser
            >>> parser = HtmlParser(Settings())
            >>> result = asyncio.run(parser.parse("http://example.com"))
        """
        # Handle URL strings directly
        if isinstance(input_path, str) and input_path.startswith(("http://", "https://")):
            return await self.parse_url(input_path, output_format=output_format, options=options)

        # Otherwise use the base implementation (expects Path)
        from pathlib import Path as _Path

        if isinstance(input_path, str):
            input_path = _Path(input_path)

        return await super().parse(input_path, output_format=output_format, options=options)

    # ---------------------------------------------------------------------
    # Validation helpers
    # ---------------------------------------------------------------------
    async def validate_input(self, input_path: Path) -> bool:
        """Validate whether input_path is a valid URL file (.url/.webloc) with an embedded URL.

        Args:
            input_path (Path): Path to link file.

        Returns:
            bool: True if file exists, is readable, and contains a valid URL.

        Example:
            >>> import asyncio
            >>> from pathlib import Path
            >>> parser = HtmlParser(Settings())
            >>> valid = asyncio.run(parser.validate_input(Path("link.url")))
            >>> print(valid)
        """
        if not input_path.exists():
            return False
        # Read file content, return False on I/O or decode errors
        try:
            content = input_path.read_text().strip()
        except (OSError, UnicodeError):
            return False
        # Check for URLs directly in content
        if content.startswith(("http://", "https://")):
            return bool(urlparse(content).netloc)
        if input_path.suffix == ".webloc":
            return "<key>URL</key>" in content
        if input_path.suffix == ".url":
            return "[InternetShortcut]" in content
        return False

    # ------------------------------------------------------------------
    # Public entry-points
    # ------------------------------------------------------------------
    async def _parse(self, input_path: Path, *, options: BaseModel | None = None) -> ParseResult:
        """Parse a local URL file and delegate to parse_url.

        Args:
            input_path (Path): Path to .url or .webloc file.
            options (BaseModel | None): Optional parser-specific options.

        Returns:
            ParseResult: Result of parse_url or error if validation fails.

        Example:
            >>> import asyncio
            >>> from pathlib import Path
            >>> parser = HtmlParser(Settings())
            >>> result = asyncio.run(parser._parse(Path("link.url")))
        """
        if not await self.validate_input(input_path):
            return ParseResult(
                content="",
                metadata=self.get_metadata(input_path),
                errors=["Invalid URL file"],
            )
        url = await self._extract_url(input_path)
        return await self.parse_url(url, output_format=None, options=options)

    async def parse_url(
        self, url: str, *, output_format: str | None = None, options: BaseModel | None = None
    ) -> ParseResult:
        """Fetch and parse a remote URL, returning structured ParseResult.

        Args:
            url (str): HTTP/HTTPS URL to fetch.
            output_format (str | None): Output format for the parsed content.
            options (BaseModel | None): Optional parser-specific options.

        Returns:
            ParseResult: Parsed content in Markdown or JSON, metadata, and errors.

        Example:
            >>> import asyncio
            >>> parser = HtmlParser(Settings())
            >>> result = asyncio.run(parser.parse_url("http://example.com"))
            >>> print(result.metadata["domain"])
        """
        try:
            content_data = await self._fetch_and_parse(url)
            effective_format = output_format or self.settings.output_format
            if effective_format == "json":
                import json as _json

                content_str = _json.dumps(content_data, indent=2, ensure_ascii=False)
            else:
                content_str = await self._format_as_markdown(content_data, url)

            metadata = {
                "url": url,
                "parser": self.__class__.__name__,
                "title": content_data.get("title", ""),
                "domain": urlparse(url).netloc,
                "content_type": content_data.get("content_type", ""),
            }
            _ = options
            return ParseResult(
                content=content_str,
                metadata=metadata,
                output_format=effective_format,
            )
        except (TimeoutError, aiohttp.ClientError, ValueError) as exc:  # pragma: no cover
            return ParseResult(content="", metadata={"url": url}, errors=[str(exc)])

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------
    async def _extract_url(self, input_path: Path) -> str:
        """Extract URL string from link file.

        Args:
            input_path (Path): Path to .url or .webloc file.

        Returns:
            str: Extracted URL string.

        Raises:
            ValueError: If URL cannot be extracted.

        Example:
            >>> url = HtmlParser(Settings())._extract_url(Path("link.url"))
        """
        content = input_path.read_text().strip()
        if content.startswith(("http://", "https://")):
            return content

        if input_path.suffix == ".webloc":
            try:
                plist = plistlib.loads(content.encode())
                return str(plist.get("URL", ""))
            except (plistlib.InvalidFileException, ValueError):
                match = re.search(r"<string>(https?://[^<]+)</string>", content)
                if match:
                    return str(match.group(1))

        if input_path.suffix == ".url":
            match = re.search(r"URL=(https?://[^\r\n]+)", content)
            if match:
                return str(match.group(1))

        raise ValueError("Could not extract URL from file")

    async def _fetch_and_parse(self, url: str) -> dict[str, Any]:
        """Fetch HTML content from URL and parse into data dictionary.

        Args:
            url (str): URL to fetch.

        Returns:
            Dict[str, Any]: Parsed data including title, description, content segments.

        Example:
            >>> import asyncio
            >>> data = asyncio.run(HtmlParser(Settings())._fetch_and_parse("http://example.com"))
            >>> print(data["title"])
        """
        async with (
            aiohttp.ClientSession() as session,
            session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as resp,
        ):
            resp.raise_for_status()
            content_type = resp.headers.get("Content-Type", "")
            html_content = await resp.text()

        soup = BeautifulSoup(html_content, "html.parser")
        title = self._extract_title(soup)
        description = self._extract_description(soup)

        is_perplexity = "perplexity.ai" in url
        if is_perplexity:
            data = await self._parse_perplexity_page(soup, url)
        else:
            data = await self._parse_general_page(soup, url)

        data.update({
            "title": title,
            "description": description,
            "content_type": content_type,
            "is_perplexity": is_perplexity,
        })
        return data

    # ---------------- specific page handlers ----------------------------
    async def _parse_perplexity_page(self, soup: BeautifulSoup, _url: str) -> dict[str, Any]:
        """Parse a Perplexity.ai page, extracting query, answer, sources, and related questions.

        Args:
            soup (BeautifulSoup): Parsed HTML soup of page.
            _url (str): Original page URL.

        Returns:
            Dict[str, Any]: Contains 'query', 'answer', 'sources', 'related_questions'.

        Example:
            >>> data = asyncio.run(HtmlParser(Settings())._parse_perplexity_page(soup, url))
            >>> assert "query" in data
        """
        data: dict[str, Any] = {
            "query": "",
            "answer": "",
            "sources": [],
            "related_questions": [],
        }

        query_elem = soup.find(["h1", "div"], class_=re.compile(r"query|question", re.I))
        if query_elem:
            data["query"] = query_elem.get_text(strip=True)

        answer_elem = soup.find(["div", "section"], class_=re.compile(r"answer|response|content", re.I))
        if answer_elem:
            data["answer"] = self.h2t.handle(str(answer_elem))
        else:
            main_content = soup.find(["main", "article", "div"], class_=re.compile(r"main|content", re.I))
            if main_content:
                data["answer"] = self.h2t.handle(str(main_content))

        if self.extract_sources:
            sources: list[dict[str, str]] = []
            for elem in soup.find_all(["a", "div"], class_=re.compile(r"source|reference|citation", re.I)):
                if not isinstance(elem, Tag):
                    continue
                href = str(elem.get("href", ""))
                if href.startswith("http"):
                    sources.append({"url": href, "title": elem.get_text(strip=True)})
            data["sources"] = sources

        for elem in soup.find_all(["div", "li"], class_=re.compile(r"related|suggestion", re.I)):
            txt = elem.get_text(strip=True)
            if txt and len(txt) > MIN_RELATED_LEN:
                data["related_questions"].append(txt)
        return data

    async def _parse_general_page(self, soup: BeautifulSoup, _url: str) -> dict[str, Any]:
        """Parse a general HTML page, extracting content, links, and images.

        Args:
            soup (BeautifulSoup): Parsed HTML soup of page.
            _url (str): Original page URL (for link resolution).

        Returns:
            Dict[str, Any]: Contains 'content', optional 'links', and 'images'.

        Example:
            >>> data = asyncio.run(HtmlParser(Settings())._parse_general_page(soup, url))
            >>> assert "content" in data
        """
        data: dict[str, Any] = {"content": "", "links": [], "images": []}

        for script in soup(["script", "style"]):
            script.decompose()

        main_content = None
        for selector in ["main", "article", '[role="main"]', ".content", "#content"]:
            main_content = soup.select_one(selector)
            if main_content:
                break
        if not main_content:
            main_content = soup.body or soup

        data["content"] = self.h2t.handle(str(main_content))

        links: list[dict[str, str]] = []
        for link in main_content.find_all("a", href=True):
            if not isinstance(link, Tag):
                continue
            href = str(link.get("href", ""))
            if href.startswith("http"):
                links.append({"url": href, "text": link.get_text(strip=True)})
        data["links"] = links[:20]

        images: list[dict[str, str]] = []
        for img in main_content.find_all("img", src=True):
            if not isinstance(img, Tag):
                continue
            images.append({"src": str(img.get("src", "")), "alt": str(img.get("alt", ""))})
        data["images"] = images[:10]
        return data

    # ---------------- markdown formatter -------------------------------
    async def _format_as_markdown(self, content_data: dict[str, Any], url: str) -> str:
        """Format parsed HTML data into Markdown.

        Builds a Markdown document with title, URL, description, separators, and sections.

        Args:
            content_data (Dict[str, Any]): Data dict returned by parsing helpers.
            url (str): Original URL string.

        Returns:
            str: Markdown-formatted string.

        Example:
            >>> md = asyncio.run(HtmlParser(Settings())._format_as_markdown(data, url))
            >>> print(md.startswith("#"))
        """
        parts: list[str] = []
        if content_data.get("title"):
            parts.append(f"# {content_data['title']}")
        parts.append(f"**URL:** {url}")
        if content_data.get("description"):
            parts.append(f"\n*{content_data['description']}*")
        parts.append("\n---\n")

        if content_data.get("is_perplexity"):
            if content_data.get("query"):
                parts.append(f"## Query\n\n{content_data['query']}")
            if content_data.get("answer"):
                parts.append(f"## Answer\n\n{content_data['answer']}")
            if content_data.get("sources") and self.extract_sources:
                parts.append("## Sources\n")
                for i, src in enumerate(content_data["sources"], 1):
                    parts.append(f"{i}. [{src['title']}]({src['url']})")
            if content_data.get("related_questions"):
                parts.append("\n## Related Questions\n")
                parts.extend(f"- {q}" for q in content_data["related_questions"])
        else:
            if content_data.get("content"):
                parts.append(content_data["content"])
            if content_data.get("links") and self.follow_links:
                parts.append("\n## Links\n")
                parts.extend(f"- [{link['text']}]({link['url']})" for link in content_data["links"][:10])
        return "\n\n".join(parts)

    # ---------------- utility extractors -------------------------------
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """Extract page title from HTML soup.

        Checks <title>, then <meta property='og:title'>, then first <h1>, fallback empty.

        Args:
            soup (BeautifulSoup): Parsed HTML soup.

        Returns:
            str: Page title or empty string.

        Example:
            >>> title = HtmlParser(Settings())._extract_title(soup)
        """
        title_tag = soup.find("title")
        if isinstance(title_tag, Tag):
            return title_tag.get_text(strip=True)
        meta_title = soup.find("meta", property="og:title")
        if isinstance(meta_title, Tag) and meta_title.get("content"):
            return str(meta_title.get("content"))
        h1 = soup.find("h1")
        if isinstance(h1, Tag):
            return h1.get_text(strip=True)
        return ""

    def _extract_description(self, soup: BeautifulSoup) -> str:
        """Extract page description from HTML soup.

        Checks <meta name='description'> then <meta property='og:description'>, fallback empty.

        Args:
            soup (BeautifulSoup): Parsed HTML soup.

        Returns:
            str: Page description or empty string.

        Example:
            >>> desc = HtmlParser(Settings())._extract_description(soup)
        """
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if isinstance(meta_desc, Tag) and meta_desc.get("content"):
            return str(meta_desc.get("content"))
        og_desc = soup.find("meta", property="og:description")
        if isinstance(og_desc, Tag) and og_desc.get("content"):
            return str(og_desc.get("content"))
        return ""



================================================================================
File: doc_parser/parsers/pdf/__init__.py
================================================================================

"""PDF parser implementation."""

from .extractors import VisionExtractor
from .parser import PDFParser

__all__ = [
    "PDFParser",
    "VisionExtractor",
]



================================================================================
File: doc_parser/parsers/pdf/extractors.py
================================================================================

"""Content extractors for PDF parsing with AI-based vision models.

This module provides VisionExtractor for extracting text content from images
using OpenAI vision models via the Agents SDK, with support for single- and
batch-mode extraction, custom prompts, and caching integration.

Classes:
    VisionExtractor: Extract text from PIL Images using vision models.

Examples:
    >>> from PIL import Image
    >>> import asyncio
    >>> from doc_parser.parsers.pdf.extractors import VisionExtractor
    >>> extractor = VisionExtractor(model_name="gpt-4o-mini")
    >>> img = Image.open("sample.png")
    >>> text = asyncio.run(extractor.extract(img))
    >>> print(text)
"""

import asyncio
import base64
from io import BytesIO
from pathlib import Path
from typing import Any

from agents import Agent, Runner
from PIL import Image

from doc_parser.core.base import BaseExtractor
from doc_parser.prompts import PromptTemplate


class VisionExtractor(BaseExtractor):
    """Extract content from images using vision models.

    Args:
        model_name (str): Name of the vision model (e.g., 'gpt-4o-mini').

    Attributes:
        model_name (str): Vision model identifier used for API calls.

    Examples:
        >>> from PIL import Image
        >>> import asyncio
        >>> from doc_parser.parsers.pdf.extractors import VisionExtractor
        >>> extractor = VisionExtractor("gpt-4o-mini")
        >>> img = Image.open("page.png")
        >>> result = asyncio.run(extractor.extract(img))
        >>> assert isinstance(result, str)
    """

    def __init__(self, model_name: str = "gpt-4o-mini"):
        """Initialize the vision extractor.

        Args:
            model_name (str): Vision model identifier for the Agents SDK.
        """
        self.model_name = model_name

    async def extract(self, content: Any, prompt_template: PromptTemplate | None = None) -> str:
        """Extract text content from one or more images using vision models.

        Args:
            content (PIL.Image.Image | List[PIL.Image.Image]): Single image or list of images.
            prompt_template (Optional[PromptTemplate | str]): Custom prompt template instance or name.

        Returns:
            str: Extracted text content, concatenated with two newlines for batches.

        Example:
            >>> from PIL import Image
            >>> import asyncio
            >>> extractor = VisionExtractor()
            >>> img = Image.open("page.png")
            >>> text = asyncio.run(extractor.extract(img))
        """
        if isinstance(content, list):
            return await self._extract_batch(content, prompt_template)
        else:
            return await self._extract_single(content, prompt_template)

    async def _extract_single(self, image: Image.Image, prompt_template: PromptTemplate | None = None) -> str:
        """Extract text from a single image.

        Args:
            image (PIL.Image.Image): Single page image.
            prompt_template (Optional[PromptTemplate | str]): Custom prompt template or name.

        Returns:
            str: Extracted text for the image.

        Example:
            >>> text = asyncio.run(extractor._extract_single(img))
        """
        prompt = self._get_prompt(prompt_template)

        # Convert image to base64
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        image_base64 = base64.b64encode(buffered.getvalue()).decode()

        # Directly call the vision model through the Agents SDK
        return await self._call_vision_api(prompt, image_base64)

    async def _extract_batch(
        self,
        images: list[Image.Image],
        prompt_template: PromptTemplate | None = None,
    ) -> str:
        """Extract text from a batch of images.

        Args:
            images (List[PIL.Image.Image]): List of page images.
            prompt_template (Optional[PromptTemplate | str]): Custom prompt template or name.

        Returns:
            str: Extracted text content for all images, joined by two newlines.

        Example:
            >>> text = asyncio.run(extractor._extract_batch([img1, img2]))
        """
        # Simply fan-out to `_extract_single` for each page and join the results.
        tasks = [self._extract_single(img, prompt_template) for img in images]
        results: list[str] = await asyncio.gather(*tasks)
        return "\n\n".join(results)

    async def _call_vision_api(self, prompt: str, image_base64: str) -> str:
        """Call the OpenAI vision API via the Agents SDK.

        Args:
            prompt (str): Prompt text to send to the model.
            image_base64 (str): Base64-encoded PNG image data.

        Returns:
            str: Raw output from the vision model.

        Raises:
            Exception: If the API call fails.

        Example:
            >>> out = await extractor._call_vision_api("prompt", base64str)
        """
        # Use the Agents SDK to run an ad-hoc Agent that performs the extraction.
        agent = Agent(
            name="VisionExtractor",
            instructions=(
                "You are a vision model that extracts text from images following the"
                " provided instructions. Respond with *only* the extracted markdown "
                "content—no additional commentary."
            ),
            model=self.model_name,
        )

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "input_text", "text": prompt},
                    {
                        "type": "input_image",
                        "image_url": f"data:image/png;base64,{image_base64}",
                        "detail": "low",
                    },
                ],
            }
        ]

        result = await Runner.run(agent, messages)  # type: ignore[arg-type]
        return str(result.final_output)

    def get_default_prompt(self) -> str:
        """Return the bundled default prompt for PDF vision extraction.

        The markdown template lives in
        ``doc_parser/prompts/templates/pdf_extraction.md``.
        """
        templates_dir = Path(__file__).resolve().parent.parent.parent / "prompts" / "templates"
        return (templates_dir / "pdf_extraction.md").read_text(encoding="utf-8")

    def _get_prompt(self, prompt_template: PromptTemplate | str | None = None) -> str:
        """Resolve and return the prompt text for extraction.

        The resolution order is:

        1. If *prompt_template* is *None*, return the default bundled template.
        2. If a :class:`PromptTemplate` instance is provided, render it.
        3. If a *str* is provided, first try to locate a markdown template file
           named ``<str>.md`` inside the bundled templates directory.  If found,
           return its contents; otherwise treat the string as a literal prompt.
        """
        if prompt_template is None:
            return self.get_default_prompt()

        if isinstance(prompt_template, PromptTemplate):
            return prompt_template.render()

        if isinstance(prompt_template, str):
            templates_dir = Path(__file__).resolve().parent.parent.parent / "prompts" / "templates"
            candidate = templates_dir / f"{prompt_template}.md"
            if candidate.exists():
                return candidate.read_text(encoding="utf-8")
            return prompt_template

        raise TypeError("prompt_template must be None, a PromptTemplate, or str")



================================================================================
File: doc_parser/parsers/pdf/parser.py
================================================================================

"""PDF parser implementation.

This module provides a parser for PDF documents that converts pages to images,
extracts text using vision models, and assembles structured output.

Features:
- Convert PDF pages to images via pdf2image
- Single and batch page extraction with async concurrency
- Caching of page results to avoid redundant extraction
- Rate limiting of API calls for controlled concurrency
- Integration with OpenAI vision models via VisionExtractor
- Configurable options: dpi, batch_size, page_range, prompt_template, output_format
- Metadata enrichment: page count, dpi, model name

Example:
>>> import asyncio
>>> from pathlib import Path
>>> from doc_parser.parsers.pdf.parser import PDFParser
>>> from doc_parser.core.settings import Settings
>>> settings = Settings(output_format="markdown", parser_settings={"pdf": {"dpi": 200, "batch_size": 2}})
>>> parser = PDFParser(settings)
>>> result = asyncio.run(parser.parse(Path("sample.pdf")))
>>> print(result.metadata["pages"], result.metadata["dpi"])
"""

from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING, cast

from pdf2image import convert_from_path
from tqdm.asyncio import tqdm

from doc_parser.config import AppConfig
from doc_parser.core.base import BaseParser, ParseResult
from doc_parser.options import PdfOptions
from doc_parser.utils.async_batcher import RateLimiter
from doc_parser.utils.cache import cache_get, cache_set

from .extractors import VisionExtractor

if TYPE_CHECKING:
    from pathlib import Path

    from PIL import Image
    from pydantic import BaseModel

    from doc_parser.prompts import PromptTemplate, PromptTemplate as _PromptTemplate


@AppConfig.register("pdf", [".pdf"])
class PDFParser(BaseParser):
    """Parser for PDF documents using image-based extraction.

    Converts PDF pages into images and extracts text via vision models,
    supporting caching and rate limiting for efficiency.

    Args:
        config (Settings): Global parser configuration.

    Attributes:
        dpi (int): Image resolution for conversion.
        batch_size (int): Number of pages per extraction batch.
        extractor (VisionExtractor): Vision-based text extractor.
        rate_limiter (RateLimiter): Controls concurrent API calls.

    Examples:
        >>> import asyncio
        >>> from pathlib import Path
        >>> from doc_parser.parsers.pdf.parser import PDFParser
        >>> from doc_parser.core.settings import Settings
        >>> settings = Settings(parser_settings={"pdf": {"dpi": 300}})
        >>> parser = PDFParser(settings)
        >>> result = asyncio.run(parser.parse(Path("doc.pdf"), page_range=(1, 3)))
        >>> print(result.content[:100])
    """

    def __init__(self, config: AppConfig):
        """Initialize PDF parser with configuration.

        Args:
            config (Settings): Parser configuration object.
        """
        super().__init__(config)

        # Get PDF-specific settings
        pdf_config = config.parser_cfg("pdf")
        self.dpi = pdf_config.get("dpi", 300)
        self.batch_size = pdf_config.get("batch_size", config.batch_size)

        # Initialize extractor
        self.extractor = VisionExtractor(model_name=config.model_name)

        # Rate limiter for API calls
        self.rate_limiter = RateLimiter(config.max_workers)

    async def validate_input(self, input_path: Path) -> bool:
        """Validate whether the input path points to a non-empty PDF file.

        Args:
            input_path (Path): Path to the PDF file.

        Returns:
            bool: True if file exists, has .pdf extension, and is non-empty.

        Example:
            >>> import asyncio
            >>> from pathlib import Path
            >>> result = asyncio.run(PDFParser(Settings()).validate_input(Path("file.pdf")))
            >>> print(result)
        """
        if not self._has_supported_extension(input_path):
            return False
        # Check if file is readable and not empty
        try:
            size = input_path.stat().st_size
        except OSError:
            return False
        return size != 0

    async def _parse(self, input_path: Path, *, options: BaseModel | None = None) -> ParseResult:  # noqa: D417
        """Parse a PDF document and return extraction results.

        Orchestrates validation, image conversion, text extraction,
        result combination, and metadata construction.

        Args:
            input_path (Path): Path to the PDF file.
            **_kwargs: Additional options:
                page_range (tuple[int, int]): Range of pages to process.
                prompt_template (PromptTemplate|str): Custom prompt template.

        Returns:
            ParseResult: Contains content, metadata, output format, and errors.

        Example:
            >>> import asyncio
            >>> from pathlib import Path
            >>> parser = PDFParser(Settings())
            >>> result = asyncio.run(parser.parse(Path("sample.pdf"), page_range=(1, 2)))
            >>> print(result.metadata["pages"])
        """
        # Validate input
        if not await self.validate_input(input_path):
            return ParseResult(
                content="",
                metadata=self.get_metadata(input_path),
                errors=[f"Invalid PDF file: {input_path}"],
            )

        # Extract typed options
        pdf_opts: PdfOptions
        if options is None:
            pdf_opts = PdfOptions()
        elif isinstance(options, PdfOptions):
            pdf_opts = options
        else:
            # Downcast - allow callers to supply generic BaseModel but ensures type safety internally.
            pdf_opts = PdfOptions.model_validate(options.model_dump())

        page_range = pdf_opts.page_range
        prompt_template = pdf_opts.prompt_template

        # Convert PDF to images
        images = await self._pdf_to_images(input_path, page_range)

        # Process pages
        results = await self._process_pages(images, input_path, cast("_PromptTemplate | None", prompt_template))

        # Combine results
        content = self._combine_results(results)

        # Build metadata
        metadata = self.get_metadata(input_path)
        metadata.update({
            "pages": len(images),
            "dpi": self.dpi,
            "model": self.settings.model_name,
        })

        return ParseResult(content=content, metadata=metadata, output_format=self.settings.output_format)

    async def _pdf_to_images(self, pdf_path: Path, page_range: tuple[int, int] | None = None) -> list[Image.Image]:
        """Convert PDF pages to PIL Image objects.

        Args:
            pdf_path (Path): Path to the PDF document.
            page_range (Optional[tuple[int, int]]): Inclusive range of pages to process.

        Returns:
            List[Image.Image]: List of page images.

        Example:
            >>> import asyncio
            >>> from pathlib import Path
            >>> images = asyncio.run(PDFParser(Settings())._pdf_to_images(Path("doc.pdf"), (1, 2)))
            >>> print(len(images))
        """
        kwargs = {
            "dpi": self.dpi,
            "fmt": "png",
            "thread_count": 4,
        }

        if page_range:
            kwargs["first_page"] = page_range[0]
            kwargs["last_page"] = page_range[1]

        # Run in thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        images = await loop.run_in_executor(None, lambda: convert_from_path(str(pdf_path), **kwargs))

        return images

    async def _process_pages(
        self,
        images: list[Image.Image],
        pdf_path: Path,
        prompt_template: PromptTemplate | None = None,
    ) -> list[str]:
        """Process page images asynchronously in batches with caching.

        Args:
            images (List[Image.Image]): List of page images.
            pdf_path (Path): Path to the source PDF.
            prompt_template (Optional[PromptTemplate]): Custom prompt or template.

        Returns:
            List[str]: Extracted text per page in original order.

        Example:
            >>> import asyncio
            >>> imgs = asyncio.run(PDFParser(Settings())._pdf_to_images(Path("doc.pdf")))
            >>> texts = asyncio.run(PDFParser(Settings())._process_pages(imgs, Path("doc.pdf")))
        """
        results = []

        # Create batches
        batches = []
        for i in range(0, len(images), self.batch_size):
            batch = images[i : i + self.batch_size]
            page_nums = list(range(i + 1, i + len(batch) + 1))
            batches.append((batch, page_nums))

        # Process batches concurrently
        tasks = []
        for batch_images, page_nums in batches:
            task = self._process_batch(batch_images, page_nums, pdf_path, prompt_template)
            tasks.append(task)

        # Execute with progress bar
        batch_results = await tqdm.gather(*tasks, desc=f"Processing {pdf_path.name}")

        # Flatten results
        for batch_result in batch_results:
            results.extend(batch_result)

        return results

    async def _process_batch(
        self,
        images: list[Image.Image],
        page_nums: list[int],
        pdf_path: Path,
        prompt_template: PromptTemplate | None = None,
    ) -> list[str]:
        """Extract content for a batch of pages and cache results.

        Args:
            images (List[Image.Image]): Subset of page images.
            page_nums (List[int]): Corresponding page numbers.
            pdf_path (Path): Source PDF path.
            prompt_template (Optional[PromptTemplate]): Custom prompt template.

        Returns:
            List[str]: Extracted content strings for the batch.

        Example:
            >>> import asyncio
            >>> result = asyncio.run(PDFParser(Settings())._process_batch([img], [1], Path("doc.pdf")))
        """
        async with self.rate_limiter:
            cached_pages, pages_to_process = await self._get_cached_pages(images, page_nums, pdf_path)

            new_results = await self._process_uncached_pages(pages_to_process, pdf_path, prompt_template)

            # Combine and sort all results to original order
            all_results = cached_pages + new_results
            all_results.sort(key=lambda tpl: tpl[0])  # sort by original index

            # Return only the extracted content
            return [content for _, content in all_results]

    # ------------------------------------------------------------------
    # Helper split-out methods to simplify _process_batch logic
    # ------------------------------------------------------------------

    async def _get_cached_pages(
        self,
        images: list[Image.Image],
        page_nums: list[int],
        pdf_path: Path,
    ) -> tuple[list[tuple[int, str]], list[tuple[int, Image.Image, int]]]:
        """Retrieve cached pages and identify pages requiring extraction.

        Args:
            images (List[Image.Image]): Page images.
            page_nums (List[int]): Page numbers for each image.
            pdf_path (Path): Source PDF path.

        Returns:
            Tuple[
                List[Tuple[int,str]],           # (index, cached_content)
                List[Tuple[int,Image.Image,int]] # (index, image, page_num)
            ]

        Example:
            >>> import asyncio
            >>> cache, to_process = asyncio.run(PDFParser(Settings())._get_cached_pages(imgs, [1], Path("doc.pdf")))
        """
        cached_pages: list[tuple[int, str]] = []
        pages_to_process: list[tuple[int, Image.Image, int]] = []

        for i, (image, page_num) in enumerate(zip(images, page_nums, strict=False)):
            cache_key = f"{pdf_path.stem}_page_{page_num}"
            cached = await cache_get(self.cache, cache_key)
            if cached:
                cached_pages.append((i, cached["content"]))
            else:
                pages_to_process.append((i, image, page_num))

        return cached_pages, pages_to_process

    async def _process_uncached_pages(
        self,
        pages_to_process: list[tuple[int, Image.Image, int]],
        pdf_path: Path,
        prompt_template: PromptTemplate | None,
    ) -> list[tuple[int, str]]:
        """Extract text for uncached pages and update cache.

        Args:
            pages_to_process (List[Tuple[int,Image.Image,int]]): Pages to extract.
            pdf_path (Path): PDF file path.
            prompt_template (Optional[PromptTemplate]): Extraction prompt.

        Returns:
            List[Tuple[int,str]]: List of (index, content) tuples.

        Example:
            >>> import asyncio
            >>> uncached = asyncio.run(PDFParser(Settings())._process_uncached_pages(to_process, Path("doc.pdf"), None))
        """
        if not pages_to_process:
            return []

        results: list[tuple[int, str]] = []

        if len(pages_to_process) == 1:
            # Single page optimisation
            idx, image, page_num = pages_to_process[0]
            content = await self.extractor.extract(image, prompt_template)
            await cache_set(
                self.cache,
                f"{pdf_path.stem}_page_{page_num}",
                {"content": content, "page": page_num},
            )
            results.append((idx, content))
            return results

        # Multiple pages: batch extract
        indices = [tpl[0] for tpl in pages_to_process]
        images = [tpl[1] for tpl in pages_to_process]
        page_numbers = [tpl[2] for tpl in pages_to_process]

        content = await self.extractor.extract(images, prompt_template)

        # TODO: If the extractor ever returns separate strings per page, split here.
        for _idx, page_num in zip(indices, page_numbers, strict=False):
            await cache_set(
                self.cache,
                f"{pdf_path.stem}_page_{page_num}",
                {"content": content, "page": page_num},
            )
        results.append((indices[0], content))

        return results

    def _combine_results(self, results: list[str]) -> str:
        """Clean and combine page extraction results into a single string.

        Joins page texts with double newlines and removes duplicate blank lines.

        Args:
            results (List[str]): List of page text strings.

        Returns:
            str: Final combined content.

        Example:
            >>> combined = PDFParser(Settings())._combine_results(["text1", "", "text2"])
        """
        # Join with double newlines
        combined = "\n\n".join(results)

        # Clean up any artifacts
        lines = combined.split("\n")
        cleaned_lines: list[str] = []

        for line in lines:
            # Skip empty lines at document boundaries
            if not line.strip() and (not cleaned_lines or not cleaned_lines[-1].strip()):
                continue
            cleaned_lines.append(line)

        return "\n".join(cleaned_lines)

    def _has_supported_extension(self, input_path: Path) -> bool:
        """Check if the input path has a supported file extension.

        Args:
            input_path (Path): Path to the file.

        Returns:
            bool: True if the file has a supported extension, False otherwise.
        """
        return super()._has_supported_extension(input_path)



================================================================================
File: doc_parser/parsers/pptx/__init__.py
================================================================================

"""PPTX parser implementation."""

from .parser import PptxParser

__all__ = [
    "PptxParser",
]



================================================================================
File: doc_parser/parsers/pptx/parser.py
================================================================================

"""PPTX parser implementation for Microsoft PowerPoint (.pptx) presentations.

This module provides a parser that extracts slide content, images, and speaker notes
from PPTX files. It supports output in Markdown or JSON formats, caching of slide
results, and configurable extraction options.

Examples:
>>> import asyncio
>>> from pathlib import Path
>>> from doc_parser.parsers.pptx.parser import PptxParser
>>> from doc_parser.core.settings import Settings
>>> settings = Settings(output_format="markdown", parser_settings={"pptx": {"extract_images": False}})
>>> parser = PptxParser(settings)
>>> result = asyncio.run(parser.parse(Path("presentation.pptx")))
>>> print(result.metadata["slides"])
"""

from __future__ import annotations

import json
from typing import TYPE_CHECKING, Any
import uuid

# Importing python-pptx objects; these are available in runtime dependencies
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE
from pptx.exc import PackageNotFoundError

from doc_parser.config import AppConfig
from doc_parser.core.base import BaseParser, ParseResult
from doc_parser.utils.cache import cache_get, cache_set
from doc_parser.utils.format_helpers import rows_to_markdown

if TYPE_CHECKING:
    from pathlib import Path

    from pydantic import BaseModel


@AppConfig.register("pptx", [".pptx"])
class PptxParser(BaseParser):
    """Parser for PowerPoint presentations (.pptx).

    Extracts slide text, tables, images, and notes into Markdown or JSON.

    Args:
        config (Settings): Global parser settings with optional 'pptx' overrides.

    Attributes:
        extract_images (bool): Whether to save and include slide images.
        extract_notes (bool): Whether to include speaker notes.
        preserve_formatting (bool): Whether to preserve text formatting.
        slide_delimiter (str): Delimiter string between slides in Markdown.

    Examples:
        >>> import asyncio
        >>> from pathlib import Path
        >>> from doc_parser.parsers.pptx.parser import PptxParser
        >>> from doc_parser.core.settings import Settings
        >>> parser = PptxParser(Settings(parser_settings={"pptx": {"slide_delimiter": "---"}}))
        >>> result = asyncio.run(parser.parse(Path("slides.pptx")))
        >>> assert result.format == "markdown"
    """

    def __init__(self, config: AppConfig):
        """Initialize PPTX parser settings.

        Args:
            config (Settings): Parser configuration object.
        """
        super().__init__(config)

        # PPTX-specific configuration
        pptx_cfg = config.parser_cfg("pptx")
        self.extract_images: bool = pptx_cfg.get("extract_images", True)
        self.extract_notes: bool = pptx_cfg.get("extract_notes", False)
        self.preserve_formatting: bool = pptx_cfg.get("preserve_formatting", True)
        self.slide_delimiter: str = pptx_cfg.get("slide_delimiter", "---")

    # ------------------------------------------------------------------
    # Validation helpers
    # ------------------------------------------------------------------
    async def validate_input(self, input_path: Path) -> bool:
        """Validate whether input_path points to a readable PPTX file.

        Args:
            input_path (Path): Path to the .pptx file.

        Returns:
            bool: True if the file exists, has a .pptx extension, and can be opened.
        """
        if not self._has_supported_extension(input_path):
            return False
        try:
            Presentation(str(input_path))
        except (PackageNotFoundError, OSError):
            return False
        return True

    # ------------------------------------------------------------------
    # Public entry-points
    # ------------------------------------------------------------------
    async def _parse(self, input_path: Path, *, options: BaseModel | None = None) -> ParseResult:
        """Parse a PPTX file and return a ParseResult.

        Iterates through slides, extracting content and caching results.

        Args:
            input_path (Path): Path to the PPTX file.
            options: Parser options object (currently unused for PPTX).

        Returns:
            ParseResult: Contains combined content, metadata, and format.
        """
        if not await self.validate_input(input_path):
            return ParseResult(
                content="",
                metadata=self.get_metadata(input_path),
                errors=[f"Invalid PPTX file: {input_path}"],
            )

        _ = options  # unused currently
        prs = Presentation(str(input_path))

        slide_markdowns: list[str] = []
        slide_dicts: list[dict[str, Any]] = []

        # Directory to save extracted images if enabled
        images_dir = self.settings.output_dir / f"{input_path.stem}_images"
        if self.extract_images:
            images_dir.mkdir(parents=True, exist_ok=True)

        # Iterate through slides
        for idx, slide in enumerate(prs.slides, start=1):
            # Include output format in cache key so markdown & JSON caches do not collide
            cache_key = f"{input_path.stem}_{self.settings.output_format}_slide_{idx}"

            cached = await cache_get(self.cache, cache_key) if self.settings.use_cache else None

            if cached:
                # Cached content reuse
                if self.settings.output_format == "json":
                    slide_dicts.append(cached["data"])
                else:
                    slide_markdowns.append(cached["content"])
                continue

            # Fresh extraction
            if self.settings.output_format == "json":
                slide_data = self._slide_to_dict(slide, images_dir)
                slide_dicts.append(slide_data)
                await cache_set(self.cache, cache_key, {"data": slide_data})
            else:
                slide_md = self._slide_to_markdown(slide, images_dir)
                slide_markdowns.append(slide_md)
                await cache_set(self.cache, cache_key, {"content": slide_md})

        # Combine slide outputs
        if self.settings.output_format == "json":
            combined_content = json.dumps(slide_dicts, indent=2, ensure_ascii=False)
        else:
            delimiter = f"\n\n{self.slide_delimiter}\n\n"
            combined_content = delimiter.join(slide_markdowns)

        metadata = self.get_metadata(input_path)
        metadata.update({"slides": len(prs.slides)})

        return ParseResult(
            content=combined_content,
            metadata=metadata,
            output_format=self.settings.output_format,
        )

    # ------------------------------------------------------------------
    # Helper methods
    # ------------------------------------------------------------------

    def _slide_to_markdown(self, slide: Any, images_dir: Path) -> str:
        """Convert a single slide to a Markdown string.

        Args:
            slide (Slide): pptx Slide object.
            images_dir (Path): Directory for saving extracted images.

        Returns:
            str: Markdown representation including title, text, tables, images, and notes.
        """
        parts: list[str] = []

        # Title placeholder
        if slide.shapes.title and slide.shapes.title.text_frame.text.strip():
            parts.append(f"# {slide.shapes.title.text_frame.text.strip()}")

        for shape in slide.shapes:
            if shape.has_table:
                parts.append(self._table_to_markdown(shape.table))
            elif shape.has_text_frame and shape is not slide.shapes.title:
                parts.extend(self._text_frame_to_markdown_lines(shape.text_frame))
            elif self.extract_images and shape.shape_type == MSO_SHAPE_TYPE.PICTURE:
                rel_path = self._save_image(shape, images_dir)
                parts.append(f"![image]({rel_path})")

        if self.extract_notes and slide.has_notes_slide and slide.notes_slide:
            note_text = slide.notes_slide.notes_text_frame.text.strip()
            if note_text:
                parts.append("\n**Notes:**\n" + note_text)

        return "\n".join(parts)

    def _slide_to_dict(self, slide: Any, images_dir: Path) -> dict[str, Any]:
        """Convert a single slide to a JSON-serializable dictionary.

        Args:
            slide (Slide): pptx Slide object.
            images_dir (Path): Directory for saving extracted images.

        Returns:
            Dict[str, Any]: Structured data with keys 'title', 'text', 'tables', 'images', 'notes'.
        """
        data: dict[str, Any] = {
            "title": slide.shapes.title.text_frame.text.strip()
            if slide.shapes.title and slide.shapes.title.text_frame.text.strip()
            else "",
            "text": [],
            "tables": [],
            "images": [],
            "notes": "",
        }

        for shape in slide.shapes:
            if shape.has_table:
                data["tables"].append(self._table_to_list(shape.table))
            elif shape.has_text_frame and shape is not slide.shapes.title:
                for para in shape.text_frame.paragraphs:
                    txt = para.text.strip()
                    if txt:
                        data["text"].append(txt)
            elif self.extract_images and shape.shape_type == MSO_SHAPE_TYPE.PICTURE:
                rel_path = self._save_image(shape, images_dir)
                data["images"].append(rel_path)

        if self.extract_notes and slide.has_notes_slide and slide.notes_slide:
            data["notes"] = slide.notes_slide.notes_text_frame.text.strip()

        return data

    # ---------------- Text helpers ----------------
    def _text_frame_to_markdown_lines(self, text_frame: Any) -> list[str]:
        lines: list[str] = []
        for para in text_frame.paragraphs:
            txt = para.text.strip()
            if not txt:
                continue
            level = para.level or 0
            prefix = ("  " * level) + ("- " if level > 0 else "")
            lines.append(prefix + txt)
        return lines

    # ---------------- Table helpers ----------------
    def _table_to_markdown(self, table: Any) -> str:
        """Convert a table shape to Markdown."""
        rows = [[cell.text_frame.text.strip() for cell in row.cells] for row in table.rows]
        return rows_to_markdown(rows)

    def _table_to_list(self, table: Any) -> list[list[str]]:
        return [[cell.text_frame.text.strip() for cell in row.cells] for row in table.rows]

    # ---------------- Image helpers ----------------
    def _save_image(self, shape: Any, images_dir: Path) -> str:
        image = shape.image
        filename = f"{uuid.uuid4().hex}.{image.ext}"
        filepath = images_dir / filename
        # Save image file
        with filepath.open("wb") as f:
            f.write(image.blob)
        # Return relative path
        return str(filepath.relative_to(self.settings.output_dir).as_posix())



================================================================================
File: doc_parser/prompts/__init__.py
================================================================================

"""Public interface for prompt templates."""

# Re-export the new **Pydantic v2** implementation
from .models import PromptTemplate

# Legacy *PromptRegistry* has been removed together with the Jinja2 implementation.
__all__: list[str] = [
    "PromptTemplate",
]



================================================================================
File: doc_parser/prompts/models.py
================================================================================

"""Core models for the ``doc_parser.prompts`` package.

This module defines :class:`PromptTemplate`, a Pydantic model that replaces the
legacy Jinja2 implementation.  The new design is intentionally lightweight:

* ``template`` - A raw Markdown string that uses Python ``str.format``
  placeholders (e.g. ``"Hello {name}!"``).
* ``input_schema`` - A *Pydantic* model **class** (not instance) that describes
  the variables the template expects. Callers may pass either a mapping or an
  instance of this schema to :py:meth:`PromptTemplate.render`; the data is
  validated automatically.
* ``output_schema`` - An optional Pydantic model **class** describing the
  expected structure of the LLM response.  Use
  :py:meth:`PromptTemplate.validate_output` to parse and validate raw model
  output.

Using Pydantic ensures strict typing and removes any runtime dependency on
Jinja2.
"""

from __future__ import annotations

from collections.abc import Mapping
from pathlib import Path
from typing import Any

from pydantic import BaseModel, Field, ValidationError, model_serializer


class PromptTemplate(BaseModel):
    """A validated, self-describing prompt template."""

    template: str = Field(..., description="Markdown prompt with str.format placeholders.")
    input_schema: type[BaseModel] = Field(..., description="Pydantic model class describing input variables.")
    output_schema: type[BaseModel] | None = Field(
        default=None,
        description="Optional Pydantic model class describing expected LLM output.",
    )

    # ------------------------------------------------------------------
    # Public helpers
    # ------------------------------------------------------------------
    def render(self, data: BaseModel | Mapping[str, Any] | None = None, **kwargs: Any) -> str:
        """Render the template after validating *data* with *input_schema*.

        Args:
            data: Either an *instance* of ``input_schema`` or a mapping that can be
                parsed into the schema. If *None*, an *empty* instance of the schema
                is created (meaning all fields must declare defaults).
            **kwargs: Extra keyword-style overrides merged *after* the validated
                data. Handy for single-use substitutions.

        Returns:
            str: The rendered prompt ready to be sent to the LLM.
        """
        # Validate / coerce *data* into the declared schema.
        if data is None:
            parsed = self.input_schema()
        elif isinstance(data, self.input_schema):
            parsed = data
        elif isinstance(data, Mapping):
            parsed = self.input_schema.model_validate(data)
        else:  # pragma: no cover - defensive
            raise TypeError(
                "data must be None, a mapping, or an instance of the declared input_schema",
            )

        context = {**parsed.model_dump(), **kwargs}
        try:
            return self.template.format(**context)
        except KeyError as exc:  # pragma: no cover - helpful error
            raise KeyError(f"Missing template variable: {exc.args[0]}") from exc

    def validate_output(self, raw_output: str) -> str | BaseModel:
        """Validate *raw_output* against ``output_schema`` if configured."""
        if self.output_schema is None:
            return raw_output
        try:
            return self.output_schema.model_validate_json(raw_output)
        except ValidationError:
            # Fallback for plain dictionary / object literals.
            return self.output_schema.model_validate(raw_output)

    # ------------------------------------------------------------------
    # Convenience constructors
    # ------------------------------------------------------------------
    @classmethod
    def from_file(
        cls,
        path: str | Path,
        input_schema: type[BaseModel],
        output_schema: type[BaseModel] | None = None,
    ) -> PromptTemplate:
        """Load *template* content from *path*.

        The file is expected to contain UTF-8 encoded text (usually ``.md``).
        """
        content = Path(path).read_text(encoding="utf-8")
        return cls(template=content, input_schema=input_schema, output_schema=output_schema)

    # ------------------------------------------------------------------
    # Model configuration
    # ------------------------------------------------------------------
    model_config = {
        "arbitrary_types_allowed": True,  # Allow BaseModel subclass types
    }

    @model_serializer(mode="plain")
    def _serialise(self) -> dict[str, Any]:
        """Custom serialiser - only *template* is included in JSON output."""
        return {"template": self.template}


# ------------------------------------------------------------------
# Public exports
# ------------------------------------------------------------------
__all__ = [
    "PromptTemplate",
]



================================================================================
File: doc_parser/utils/__init__.py
================================================================================

"""Utility modules for the document parser library."""

from .async_batcher import AsyncBatcher, RateLimiter
from .cache import CacheManager

__all__ = [
    "AsyncBatcher",
    "CacheManager",
    "RateLimiter",
]



================================================================================
File: doc_parser/utils/async_batcher.py
================================================================================

"""Asynchronous batching utility.

This module provides AsyncBatcher, which collects items into batches up to a specified
batch_size or timeout, and processes them via a user-supplied coroutine function.

Examples:
    >>> import asyncio
    >>> from doc_parser.utils.async_batcher import AsyncBatcher
    >>> async def process(batch):
    ...     return [item * 2 for item in batch]
    >>> batcher = AsyncBatcher(batch_size=3, process_func=process, timeout=0.1)
    >>> async def run():
    ...     results = await asyncio.gather(*[batcher.add(i) for i in [1, 2, 3, 4]])
    ...     print(results)
    >>> asyncio.run(run())
    [2, 4, 6, 8]
"""

from __future__ import annotations

import asyncio
from collections import deque
from typing import TYPE_CHECKING, Any, TypeVar, cast

if TYPE_CHECKING:
    from collections.abc import Awaitable, Callable

T = TypeVar("T")


class AsyncBatcher:  # pylint: disable=too-many-instance-attributes
    """Batch asynchronous operations for efficiency.

    Collects items into batches and invokes the provided process_func coroutine,
    enabling efficient bulk operations with concurrency control.

    Args:
        batch_size (int): Maximum number of items per batch.
        process_func (Callable[[List[T]], Awaitable[List[Any]]]): Coroutine to process each batch.
        timeout (float): Maximum seconds to wait before processing a partial batch.

    Attributes:
        batch_size (int)
        process_func (Callable)
        timeout (float)

    Example:
        >>> import asyncio
        >>> async def proc(items):
        ...     return [i + 1 for i in items]
        >>> batcher = AsyncBatcher(2, proc)
        >>> results = asyncio.run(asyncio.gather(batcher.add(1), batcher.add(2)))
        >>> print(results)
        [2, 3]
    """

    def __init__(
        self,
        batch_size: int | None = None,
        process_func: Callable[[list[T]], Awaitable[list[Any]]] | None = None,
        timeout: float = 1.0,
        max_concurrent: int | None = None,
        show_progress: bool = True,
    ) -> None:
        """Create an AsyncBatcher.

        This unified helper can be used in **two** distinct modes:

        1. *Batch mode* - supply ``batch_size`` **and** ``process_func`` then call
           :py:meth:`add` to stream items that will be processed in batches.
        2. *Gather mode* - omit ``batch_size``/``process_func`` and call
           :py:meth:`gather` with an iterable of awaitable *callables* (or
           awaitables) to run them with optional concurrency limiting and a
           progress bar.

        Args:
            batch_size: Maximum number of items per batch. Required for *batch
                mode*.
            process_func: Coroutine to process a list of items. Required for
                *batch mode*.
            timeout: Maximum seconds to wait before processing a partial batch.
            max_concurrent: Limit on simultaneously awaited tasks when using
                :py:meth:`gather` or when this object is used as an async
                context-manager.
            show_progress: Whether to render a progress bar using
                ``tqdm.asyncio`` when calling :py:meth:`gather`.
        """
        # --- Batch-specific attributes
        self.batch_size = batch_size
        self.process_func = process_func
        self.timeout = timeout

        # Internal state for batch mode
        self._queue: deque[tuple[int, Any, asyncio.Future[Any]]] = deque()
        self._lock = asyncio.Lock()
        self._task: asyncio.Task[Any] | None = None

        # --- Concurrency control
        self._semaphore: asyncio.Semaphore | None = asyncio.Semaphore(max_concurrent) if max_concurrent else None

        # UI helpers
        self._show_progress = show_progress

    async def add(self, item: T) -> Any:
        """Add an item to the batch and await its processed result.

        This method is only available in *batch mode* (when both
        ``batch_size`` and ``process_func`` were provided at construction
        time). Attempting to call it otherwise will raise ``RuntimeError``.

        Args:
            item: Item to be queued for processing.

        Returns:
            The result corresponding to the provided *item* once
            ``process_func`` has processed the batch it belongs to.
        """
        if self.batch_size is None or self.process_func is None:
            raise RuntimeError("'add' can only be used when 'batch_size' and 'process_func' are set.")

        item_id = id(item)
        future: asyncio.Future[Any] = asyncio.Future()

        async with self._lock:
            self._queue.append((item_id, item, future))

            if self._task is None or self._task.done():
                self._task = asyncio.create_task(self._process_batches())

        return await future

    # ------------------------------------------------------------------
    # Internal machinery
    # ------------------------------------------------------------------
    async def _process_batches(self) -> None:
        """Internal loop to process batches of items.

        Continuously collects items up to batch_size or until timeout,
        invokes process_func on the batch, and sets each Future with its result.
        """
        # Assert required attributes for batch mode to satisfy type checker.
        assert self.batch_size is not None, "batch_size should be set in batch mode"  # noqa: S101
        assert self.process_func is not None, "process_func should be set in batch mode"  # noqa: S101

        while self._queue:
            batch: list[Any] = []
            futures: list[tuple[int, asyncio.Future[Any]]] = []

            async with self._lock:
                while self._queue and len(batch) < self.batch_size:
                    item_id, item, future = self._queue.popleft()
                    batch.append(item)
                    futures.append((item_id, future))

            if batch:
                try:
                    results = await self.process_func(batch)
                    for i, (_, future) in enumerate(futures):
                        if i < len(results):
                            future.set_result(results[i])
                        else:
                            future.set_exception(Exception("No result for item"))
                except (RuntimeError, ValueError) as exc:  # pragma: no cover
                    for _, future in futures:
                        future.set_exception(exc)

            await asyncio.sleep(0.01)

    # ------------------------------------------------------------------
    # Concurrency helpers
    # ------------------------------------------------------------------

    async def __aenter__(self) -> AsyncBatcher:
        """Enter the asynchronous context.

        If *max_concurrent* was supplied, this acquires the semaphore slot so
        that nested awaits can be rate-limited. When *max_concurrent* is *None*
        the context-manager is effectively a no-op.
        """
        if self._semaphore is not None:
            await self._semaphore.acquire()
        return self

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc_val: BaseException | None,
        exc_tb: object | None,
    ) -> None:
        """Exit the asynchronous context and release any acquired semaphore."""
        if self._semaphore is not None:
            self._semaphore.release()

    async def gather(
        self,
        tasks: list[Callable[[], Awaitable[Any]]] | list[Awaitable[Any]],
        *,
        desc: str = "Processing",
    ) -> list[Any]:
        """Concurrently run *tasks* with optional throttling & progress bar.

        Accepts either a list of *awaitables* **or** a list of zero-argument
        async callables. The latter form enables internal semaphore wrapping
        without forcing the caller to write wrapper lambdas themselves.
        """
        from tqdm.asyncio import tqdm  # Local import to avoid heavy dep at runtime

        awaitables: list[Awaitable[Any]] = []

        if not tasks:
            return []

        # Detect whether the first element is awaitable or callable
        first = tasks[0]

        if self._semaphore is None:
            # No concurrency constraint; convert tasks directly
            awaitables = [t() if callable(t) else t for t in tasks]
        # Wrap each task in semaphore guard if callable
        elif callable(first):

            async def _wrap(task: Callable[[], Awaitable[Any]]) -> Any:
                async with self:
                    return await task()

            awaitables = [_wrap(cast("Callable[[], Awaitable[Any]]", t)) for t in tasks]
        else:
            # Provided items are awaitables; we cannot wrap easily - we assume
            # caller wants no additional throttling. Warn via RuntimeError.
            raise RuntimeError(
                "When using 'gather' with 'max_concurrent', provide a list of callables, not awaitables."
            )

        if self._show_progress:
            # tqdm.asyncio.gather accepts **kwargs such as desc
            return await tqdm.gather(*awaitables, desc=desc)

        # Fall back to standard asyncio.gather (no desc parameter)
        return await asyncio.gather(*awaitables)

    # ------------------------------------------------------------------
    # Static utilities (moved from async_helpers)
    # ------------------------------------------------------------------

    @staticmethod
    async def run_with_retry(
        func: Callable[..., Awaitable[Any]],
        *args: Any,
        max_retries: int = 3,
        backoff_factor: float = 2.0,
        retry_exceptions: tuple[type[Exception], ...] = (
            RuntimeError,
            ValueError,
            ConnectionError,
            TimeoutError,
        ),
        **kwargs: Any,
    ) -> Any:
        """Run *func* with retry logic and exponential backoff.

        This is a lift-and-shift of ``run_with_retry`` from the previous
        *async_helpers* module so callers can simply switch imports.
        """
        last_exception: Exception | None = None

        for attempt in range(max_retries + 1):
            try:
                return await func(*args, **kwargs)
            except retry_exceptions as exc:
                last_exception = exc
                if attempt < max_retries:
                    wait_time = backoff_factor**attempt
                    await asyncio.sleep(wait_time)
                else:
                    raise last_exception from last_exception


# ----------------------------------------------------------------------
# Backwards-compat alias for legacy RateLimiter import sites
# ----------------------------------------------------------------------


class RateLimiter:  # pylint: disable=too-few-public-methods
    """Backwards-compatibility wrapper emulating the original *RateLimiter*.

    Internally this delegates to :class:`AsyncBatcher` configured with only
    the *max_concurrent* semaphore limiter. Users can therefore keep their
    existing ``async with RateLimiter(...):`` callsites unchanged.
    """

    def __init__(self, max_concurrent: int = 10) -> None:
        """Create a *RateLimiter* wrapper.

        Args:
            max_concurrent: Maximum concurrent operations permitted inside the
                managed context.
        """
        self._limiter = AsyncBatcher(max_concurrent=max_concurrent)

    async def __aenter__(self) -> RateLimiter:
        """Enter the *RateLimiter* context by delegating to the internal limiter."""
        await self._limiter.__aenter__()
        return self

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc_val: BaseException | None,
        exc_tb: object | None,
    ) -> None:
        """Exit the context and release the internal semaphore."""
        await self._limiter.__aexit__(exc_type, exc_val, exc_tb)


# ------------------------------------------------------------------
# Public exports
# ------------------------------------------------------------------
__all__ = [
    "AsyncBatcher",
    "RateLimiter",
]



================================================================================
File: doc_parser/utils/cache.py
================================================================================

"""Caching utilities for parsed documents.

This module provides CacheManager for persistent JSON-based caching of parser results,
with optional TTL support, and lightweight functional helpers for simple cache operations.

Classes:
    CacheManager: Manages cache entries with TTL, metadata, and file I/O.

Functions:
    cache_get(manager, key): Async helper to retrieve a cached entry.
    cache_set(manager, key, data): Async helper to store a cache entry.

Examples:
    >>> import asyncio
    >>> from pathlib import Path
    >>> from datetime import timedelta
    >>> from doc_parser.utils.cache import CacheManager, cache_set, cache_get
    >>> cm = CacheManager(Path("cache_dir"), ttl=timedelta(seconds=60))
    >>> await cache_set(cm, "test", {"foo": "bar"})
    >>> data = await cache_get(cm, "test")
    >>> print(data)
    {'foo':'bar'}
"""

import asyncio
from datetime import datetime, timedelta
import json
from pathlib import Path
from typing import Any, Any as _Any, cast

import aiofiles

from doc_parser.core.exceptions import CacheError


class CacheManager:
    """Manages JSON file caching for parsed documents with optional expiration (TTL).

    Attributes:
        cache_dir (Path): Directory where cache files are stored (auto-created).
        ttl (Optional[timedelta]): Time-to-live for entries; None for no expiration.

    Methods:
        get(key) -> Optional[Dict[str, Any]]: Retrieve cached data or None if missing/expired.
        set(key, data): Store data under the key with metadata.
        delete(key): Remove cache entry and metadata.
        clear(): Delete all cache files in the cache_dir.
        get_size() -> int: Return total size of cache files in bytes.

    Examples:
        >>> import asyncio
        >>> from pathlib import Path
        >>> from datetime import timedelta
        >>> from doc_parser.utils.cache import CacheManager
        >>> cm = CacheManager(Path("cache"), ttl=timedelta(minutes=5))
        >>> await cm.set("a", {"x": 1})
        >>> d = await cm.get("a")
        >>> print(d)
        {'x':1}
    """

    def __init__(self, cache_dir: Path, ttl: timedelta | None = None):
        """Initialize cache manager.

        Args:
            cache_dir: Directory to store cache files
            ttl: Time to live for cache entries (None for no expiration)
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.ttl = ttl
        self._lock = asyncio.Lock()

    def _get_cache_path(self, key: str) -> Path:
        """Get path for cache file."""
        return self.cache_dir / f"{key}.json"

    def _get_metadata_path(self, key: str) -> Path:
        """Get path for cache metadata file."""
        return self.cache_dir / f"{key}.meta.json"

    async def get(self, key: str) -> dict[str, Any] | None:
        """Retrieve cached data for a given key.

        Args:
            key (str): Unique cache key.

        Returns:
            Optional[Dict[str, Any]]: Cached data dict or None if missing/expired.

        Example:
            >>> data = await cm.get("test")
        """
        cache_path = self._get_cache_path(key)
        meta_path = self._get_metadata_path(key)

        if not cache_path.exists():
            return None

        try:
            # Check expiration
            if self.ttl and meta_path.exists():
                async with aiofiles.open(meta_path) as f:
                    metadata = json.loads(await f.read())

                created = datetime.fromisoformat(metadata["created"])
                if datetime.now() - created > self.ttl:
                    await self.delete(key)
                    return None

            # Read cached data
            async with aiofiles.open(cache_path) as f:
                data = json.loads(await f.read())

            return cast("dict[str, Any]", data)
        except (OSError, json.JSONDecodeError) as e:
            raise CacheError(f"Failed to read cache: {e}") from e

    async def set(self, key: str, data: dict[str, Any]) -> None:
        """Persist *data* in *manager* under *key*."""
        cache_path = self._get_cache_path(key)
        meta_path = self._get_metadata_path(key)

        try:
            async with self._lock:
                # Write data
                async with aiofiles.open(cache_path, "w") as f:
                    await f.write(json.dumps(data, indent=2, default=str))

                # Write metadata
                metadata = {
                    "created": datetime.now().isoformat(),
                    "key": key,
                }
                async with aiofiles.open(meta_path, "w") as f:
                    await f.write(json.dumps(metadata, indent=2))
        except (OSError, TypeError) as e:
            raise CacheError(f"Failed to write cache: {e}") from e

    async def delete(self, key: str) -> None:
        """Delete cached data."""
        cache_path = self._get_cache_path(key)
        meta_path = self._get_metadata_path(key)

        if cache_path.exists():
            cache_path.unlink()
        if meta_path.exists():
            meta_path.unlink()

    async def clear(self) -> None:
        """Clear all cached data."""
        for path in self.cache_dir.glob("*.json"):
            path.unlink()

    async def get_size(self) -> int:
        """Get total cache size in bytes."""
        return sum(p.stat().st_size for p in self.cache_dir.glob("*.json"))


# ---------------------------------------------------------------------------
# Lightweight functional helpers - preferred over calling ``CacheManager``
# methods directly from client code.  They keep call-sites concise and decouple
# them from the underlying implementation should we swap backends in the future.
# ---------------------------------------------------------------------------


async def cache_get(manager: "CacheManager", key: str) -> dict[str, Any] | None:
    """Async helper to return cached data for a key using the specified manager.

    Args:
        manager (CacheManager): CacheManager instance.
        key (str): Cache key.

    Returns:
        Optional[Dict[str, Any]]: Cached data or None.

    Example:
        >>> data = await cache_get(cm, "test")
    """
    return await manager.get(key)


async def cache_set(manager: "CacheManager", key: str, data: dict[str, _Any]) -> None:
    """Async helper to store data in the cache under the specified key.

    Args:
        manager (CacheManager): CacheManager instance.
        key (str): Cache key.
        data (Dict[str, Any]): JSON-serializable data to cache.

    Example:
        >>> await cache_set(cm, "test", {"foo": "bar"})
    """
    await manager.set(key, data)


# ------------------------------------------------------------------
# Public exports
# ------------------------------------------------------------------
__all__ = [
    "CacheManager",
    "cache_get",
    "cache_set",
]



================================================================================
File: doc_parser/utils/format_helpers.py
================================================================================

"""Formatting helpers for converting structured data to Markdown.

This module provides utilities to render rows, DataFrames, and lists into
GitHub-flavored Markdown tables, handling escaping and basic heuristics.

Functions:
    rows_to_markdown(rows: Sequence[Sequence[str]]) -> str
    dataframe_to_markdown(df: pd.DataFrame) -> str

Examples:
    >>> from doc_parser.utils.format_helpers import rows_to_markdown
    >>> rows = [["Name", "Age"], ["Alice", "30"], ["Bob", "25"]]
    >>> print(rows_to_markdown(rows))
    | Name  | Age |
    | ----- | --- |
    | Alice | 30  |
    | Bob   | 25  |
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import pandas as pd

if TYPE_CHECKING:
    from collections.abc import Sequence

# ---------------------------------------------------------------------------
# Markdown table helpers
# ---------------------------------------------------------------------------


def rows_to_markdown(rows: Sequence[Sequence[str]]) -> str:
    """Convert rows (header + data) into a GitHub-flavored Markdown table.

    The first row in *rows* is treated as the header.

    Args:
        rows (Sequence[Sequence[str]]): List of rows, each a sequence of cell strings.

    Returns:
        str: GitHub-flavored Markdown table.

    Example:
        >>> rows = [["A", "B"], ["1", "2"]]
        >>> print(rows_to_markdown(rows))
        | A | B |
        | - | - |
        | 1 | 2 |
    """
    rows = [list(map(_escape_cell, r)) for r in rows]
    if not rows:
        return ""

    header = rows[0]
    lines: list[str] = []
    lines.append("| " + " | ".join(header) + " |")
    lines.append("| " + " | ".join(["-" * max(3, len(h)) for h in header]) + " |")

    # Use extend for performance when adding rows
    lines.extend(f"| {' | '.join(row)} |" for row in rows[1:])
    return "\n".join(lines)


def dataframe_to_markdown(df: pd.DataFrame) -> str:
    """Render a pandas DataFrame as a GitHub-flavored Markdown table.

    Detects header row heuristically and escapes pipe characters.

    Args:
        df (pd.DataFrame): DataFrame to render.

    Returns:
        str: Markdown table string, or '*No data*' if DataFrame is empty.

    Example:
        >>> import pandas as pd
        >>> from doc_parser.utils.format_helpers import dataframe_to_markdown
        >>> df = pd.DataFrame({"X": [1, 2], "Y": [3, 4]})
        >>> md = dataframe_to_markdown(df)
        >>> md.startswith("| X | Y |")
        True
    """
    if df.empty:
        return "*No data*"

    # pick header row heuristically: first row with non-nulls
    header_row = 0
    for i in range(min(5, len(df))):
        row = df.iloc[i]
        if all(pd.notna(cell_value) and str(cell_value).strip() for cell_value in row[:5]):
            header_row = i
            break

    if header_row > 0:
        headers = df.iloc[header_row].astype(str).tolist()
        data_df = df.iloc[header_row + 1 :].reset_index(drop=True)
        data_df.columns = headers
    else:
        headers = [f"Column {i + 1}" for i in range(len(df.columns))]
        data_df = df.copy()
        data_df.columns = headers

    rows: list[list[str]] = [headers]
    for _, row in data_df.iterrows():
        cells = ["" if pd.isna(v) else _escape_cell(str(v)) for v in row]
        rows.append(cells)

    return rows_to_markdown(rows)


# ---------------------------------------------------------------------------
# Internal helpers
# ---------------------------------------------------------------------------


def _escape_cell(text: str) -> str:
    r"""Escape pipe and newline characters inside table cells.

    Args:
        text (str): Cell text to escape.

    Returns:
        str: Escaped text safe for Markdown tables.

    Example:
        >>> from doc_parser.utils.format_helpers import _escape_cell
        >>> _escape_cell("a|b\nc")
        'a\\|b c'
    """
    return text.replace("|", "\\|").replace("\n", " ")


# ------------------------------------------------------------------
# Public exports
# ------------------------------------------------------------------
__all__ = [
    "dataframe_to_markdown",
    "rows_to_markdown",
]



================================================================================
File: doc_parser/utils/llm_post_processor.py
================================================================================

"""LLM-based post-processing for parsed content.

This module provides LLMPostProcessor to perform a secondary LLM call using the OpenAI Agents SDK,
applying custom prompts or templates, with optional caching and structured response support.

Classes:
    LLMPostProcessor: Handles post-parse prompting, caching, and optional Pydantic model coercion.

Examples:
    >>> import asyncio
    >>> from pathlib import Path
    >>> from doc_parser.config import AppConfig
    >>> from doc_parser.utils.llm_post_processor import LLMPostProcessor
    >>> settings = Settings(use_cache=False)
    >>> processor = LLMPostProcessor(settings)
    >>> result = asyncio.run(processor.process("raw text", "Summarize content"))
    >>> print(result)
"""

from __future__ import annotations

import hashlib
import importlib
import json
from pathlib import Path
from typing import TYPE_CHECKING, Any

from agents import Agent, Runner

from doc_parser.config import AppConfig
from doc_parser.prompts import PromptTemplate  # noqa: F401 - imported for type hints / backwards-compat
from doc_parser.utils.cache import CacheManager, cache_get, cache_set

if TYPE_CHECKING:
    from doc_parser.config import AppConfig

try:
    from pydantic import BaseModel
except ImportError:  # pragma: no cover
    BaseModel = object  # type: ignore


class LLMPostProcessor:
    """Performs secondary LLM-based post-processing of parsed content.

    Args:
        config (Settings): Global configuration including response_model and caching.
        cache_manager (Optional[CacheManager]): Custom cache manager; defaults to one based on config.cache_dir.

    Attributes:
        config (Settings): Parser and post-processing settings.
        cache (CacheManager): Cache manager for post-processing results.

    Examples:
        >>> import asyncio
        >>> from pathlib import Path
        >>> from doc_parser.config import AppConfig
        >>> from doc_parser.utils.llm_post_processor import LLMPostProcessor
        >>> settings = Settings(use_cache=True, response_model=None)
        >>> processor = LLMPostProcessor(settings)
        >>> output = asyncio.run(processor.process("content", "Summarize"))
        >>> print(output)
    """

    def __init__(self, config: AppConfig, cache_manager: CacheManager | None = None):
        """Create a new post-processor.

        Args:
            config (Settings): Application settings containing post-processing options.
            cache_manager (CacheManager | None): Optional cache override. If omitted a
                new `CacheManager` is created from ``config.cache_dir``.
        """
        self.config: AppConfig = config
        self.cache = cache_manager or CacheManager(Path(config.cache_dir))

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    async def process(self, primary_content: str, post_prompt: str) -> Any:
        """Post-process parsed content using a second LLM call.

        Args:
            primary_content (str): Main content to be post-processed.
            post_prompt (str): Prompt template name or literal prompt for LLM.

        Returns:
            Any: Post-processed content, which may be a string or Pydantic model instance if configured.

        Raises:
            Exception: If LLM call or caching fails unexpectedly.

        Example:
            >>> output = await processor.process("Hello world", "Translate to French")
        """
        resolved_prompt = self._resolve_prompt(post_prompt)

        cache_key = self._make_cache_key(primary_content, resolved_prompt)
        if self.config.use_cache:
            cached = await cache_get(self.cache, cache_key)
            if cached:
                cached_content = cached.get("post_content")
                # If a structured response model is configured and the cached value
                # is a JSON string, convert it back into the Pydantic object so that
                # callers always receive the same rich type that a live LLM call
                # would return.
                if self.config.response_model and isinstance(cached_content, str):
                    try:
                        model_cls = self._import_response_model(self.config.response_model)
                        if issubclass(model_cls, BaseModel):
                            return model_cls.model_validate_json(cached_content)
                    except (ValueError, TypeError):  # pragma: no cover
                        pass  # fall through to return cached string
                return cached_content

        # Call LLM (placeholder implementation)
        post_content = await self._call_llm(resolved_prompt, primary_content)

        # If we expect a structured object but received a JSON/string, coerce now
        if self.config.response_model:
            model_cls = self._import_response_model(self.config.response_model)
            if model_cls and issubclass(model_cls, BaseModel) and not isinstance(post_content, model_cls):
                post_content = model_cls.model_validate_json(str(post_content))

        # Cache - store something that can be JSON-encoded
        if self.config.use_cache and post_content is not None:
            serialisable = post_content.model_dump_json() if isinstance(post_content, BaseModel) else post_content
            await cache_set(self.cache, cache_key, {"post_content": serialisable})

        return post_content

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------
    def _resolve_prompt(self, prompt_or_name: str) -> str:
        """Resolve *prompt_or_name* into a final prompt string.

        The lookup strategy is now filesystem-based:

        1. If *prompt_or_name* corresponds to a file in
           ``doc_parser/prompts/templates/<name>.md`` the file contents are
           returned.
        2. Otherwise the input string is treated as a *literal* prompt.

        This approach removes the legacy PromptRegistry dependency while still
        supporting convenient shorthand names for bundled templates.
        """
        templates_dir = Path(__file__).resolve().parent.parent / "prompts" / "templates"
        candidate = templates_dir / f"{prompt_or_name}.md"
        if candidate.exists():
            return candidate.read_text(encoding="utf-8")
        return prompt_or_name

    def _make_cache_key(self, primary_content: str, prompt: str) -> str:
        """Generate a stable cache key based on content and prompt.

        Args:
            primary_content (str): Original parsed content.
            prompt (str): Resolved prompt text.

        Returns:
            str: SHA256 hex digest for caching.

        Example:
            >>> key = processor._make_cache_key("data", "prompt")
        """
        key_data = {
            "primary": primary_content,
            "prompt": prompt,
            "model": self.config.response_model,
        }
        key_str = json.dumps(key_data, sort_keys=True, default=str)
        return hashlib.sha256(key_str.encode()).hexdigest()

    async def _call_llm(self, prompt: str, content: str) -> str:
        """Call an LLM (via Agents SDK) **or** fall back to a deterministic stub.

        The behaviour is now as follows:

        1. **Live mode** - If an ``OPENAI_API_KEY`` environment variable is
           present we *attempt* a real call using the Agents SDK.  This is
           opt-in so that CI and unit tests remain fully deterministic.
        2. **Offline mode** - When the API key is missing **or** a live call
           fails for *any* reason, we gracefully fall back to the original
           stub logic that guarantees repeatable output.

        This hybrid strategy keeps the test-suite fast and reliable while
        allowing end-users to benefit from high-fidelity post-processing when
        credentials are provided locally.

        Args:
            prompt (str): The user/system prompt (may include a JSON schema).
            content (str): Primary content to send to the LLM.

        Returns:
            str: The raw LLM output (string or JSON) which will later be
                 coerced into a Pydantic model if ``response_model`` is set.
        """
        import os

        # Require an API key - fail fast if not configured to avoid silent
        # fall-throughs that yield unusable output.
        if not os.getenv("OPENAI_API_KEY"):
            raise RuntimeError(
                "OPENAI_API_KEY not found in environment. Set it (e.g. via a .env "
                "file or shell export) to enable LLM post-processing."
            )

        system_prompt, output_type = self._build_system_prompt(prompt)
        return await self._run_agent(system_prompt, content, output_type)

    def _build_system_prompt(self, prompt: str) -> tuple[str, type | None]:
        """Build the system prompt and determine the output type based on config.

        Args:
            prompt (str): Base prompt text or template.

        Returns:
            Tuple[str, type | None]: System prompt with schema and optional Pydantic type.

        Example:
            >>> sys, typ = processor._build_system_prompt("Prompt")
        """
        system_prompt = prompt
        output_type: type | None = None

        if self.config.response_model:
            try:
                model_cls = self._import_response_model(self.config.response_model)
                if model_cls and issubclass(model_cls, BaseModel):
                    schema_json = json.dumps(model_cls.model_json_schema(), indent=2)
                    system_prompt = (
                        f"{prompt}\n\n"
                        "When you respond, output ONLY a JSON object that strictly matches "
                        "the following JSON schema. Do not wrap the JSON in markdown or "
                        "any additional text.\n\n"
                        f"{schema_json}"
                    )
                    output_type = model_cls
            except (ValueError, TypeError):  # pragma: no cover
                # Ignore schema embedding on failure, proceed with plain prompt
                output_type = None

        return system_prompt, output_type

    async def _run_agent(self, system_prompt: str, content: str, output_type: type | None) -> str:
        """Execute an Agents SDK Agent with system prompt and input content.

        Args:
            system_prompt (str): Instructions for the agent.
            content (str): Input content to process.
            output_type (Optional[type]): Expected output type for structured responses.

        Returns:
            str: Agent's final output as string.

        Example:
            >>> out = await processor._run_agent("sys", "content", None)
        """
        agent = Agent(
            name="PostProcessor",
            instructions=system_prompt,
            model=self.config.model_name,
            output_type=output_type,
        )

        result = await Runner.run(agent, content)

        # If a structured model instance was returned, serialise to JSON so that
        # downstream ``model_validate_json`` receives valid input rather than a
        # Python repr string such as ``authors=['...', ...]``.
        final = result.final_output
        try:
            if output_type and isinstance(final, BaseModel):
                return final.model_dump_json()
        except ImportError:  # pragma: no cover - pydantic always installed in lib
            pass

        return str(final)

    def _import_response_model(self, import_path: str) -> Any:
        """Dynamically import and return a class from an import path.

        Args:
            import_path (str): Dotted or colon-separated module path to a class.

        Returns:
            Any: Imported class or attribute.

        Raises:
            ImportError: If the path is invalid or the attribute not found.

        Example:
            >>> cls = processor._import_response_model("mypkg.models:MyModel")
        """
        module_path, _, attr = import_path.partition(":") if ":" in import_path else import_path.rpartition(".")
        if not module_path or not attr:
            raise ImportError("Invalid import path for response model")
        module = importlib.import_module(module_path)
        return getattr(module, attr)


# ------------------------------------------------------------------
# Public exports
# ------------------------------------------------------------------
__all__ = [
    "LLMPostProcessor",
]



================================================================================
File: examples/example_usage.py
================================================================================

#!/usr/bin/env python3
# mypy: ignore-errors

"""Example usage of the document parser library.

This script is illustrative and not part of the library's typed public API,
so we opt out of strict mypy checking to avoid polluting CI results.
"""

import asyncio
from pathlib import Path

from doc_parser import parsers  # noqa: F401
from doc_parser.config import AppConfig


async def example_basic_usage():
    """Basic usage example."""
    # Create configuration
    config = AppConfig(
        max_workers=10,
        model_name="gpt-4.1-mini",
        output_format="markdown",
        parser_settings={"pdf": {"dpi": 300, "batch_size": 1}},
        use_cache=False,
    )

    # Parse a PDF file (using the existing hello.py as test)
    pdf_path = Path(
        "/Users/joneickmeier/Documents/Papers Library/Monteggia-The best way forward-2014-Nature.pdf"
    )
    if pdf_path.exists():
        try:
            parser = AppConfig.from_path(pdf_path, config)
            result = await parser.parse(pdf_path)


            # Save to markdown file via ParseResult convenience method
            output_path = Path("outputs") / f"{pdf_path.stem}_parsed.md"
            result.save_markdown(output_path)

        except Exception:
            pass



async def example_html_usage():
    """Example usage of HTML parser to parse a URL."""
    # Create configuration
    config = AppConfig(
        max_workers=10, model_name="gpt-4.1-mini", output_format="markdown"
    )

    url = "https://www.theglobeandmail.com/"

    try:
        parser = AppConfig.from_path(url, config)
        # Directly parse the URL without creating any temporary files
        result = await parser.parse(url)


        # Save to markdown file
        # Construct a safe filename from the URL to avoid invalid path characters
        safe_url = url.replace("/", "_").replace(":", "_")
        output_path = Path("outputs") / f"{safe_url}_parsed.md"
        result.save_markdown(output_path)

    except Exception:
        pass



async def main():
    """Run all examples."""
    await example_basic_usage()
    await example_html_usage()



if __name__ == "__main__":
    asyncio.run(main())



================================================================================
File: examples/post_processing_example.py
================================================================================

"""Example: post-parse prompting programmatic usage."""

import asyncio
import logging
from pathlib import Path

from dotenv import load_dotenv
from pydantic import BaseModel, ConfigDict, Field, field_validator

from doc_parser.config import AppConfig

logging.basicConfig(level=logging.INFO)

class QuestionAnswer(BaseModel):
    """A model representing a question and its corresponding answer extracted from a document.

    Usage:
        >>> qa = QuestionAnswer(question='What is AI?', answer='AI is artificial intelligence')
        >>> print(qa)
        question='What is AI?' answer='AI is artificial intelligence'
    """

    question: str = Field(description="The question in the document")
    answer: str = Field(description="The answer to the question")


class DocumentSummary(BaseModel):
    """A model capturing metadata and detailed summary of a document.

    Attributes:
        authors (list[str]): Who is the author(s) of the document.
        title (str): Title of the document.
        date (Optional[str]): Date of the document.
        publisher (Optional[str]): Publisher of the document.
        document_type (str): Type of the document.
        summary (str): A brief summary of the document.
        topic_timelessness (str): Whether the topic is timeless or time sensitive.
        keywords (list[str]): Key keywords or themes from the document.
        key_points (list[str]): List of key points and takeaways.
        key_questions (list[QuestionAnswer]): List of key questions and answers.
        key_recommendations (list[str]): List of key recommendations.
        key_risks (list[str]): List of key risks.
        key_assumptions (list[str]): List of key assumptions.

    Usage:
        >>> data = {
        ...     'Author': ['Alice', 'Bob'],
        ...     'Title': 'Example Doc',
        ...     'Document Type': 'blog_post',
        ...     'Summary': 'This document discusses important topics.',
        ...     'Topic Timelessness': 'timeless',
        ...     'Keywords': ['AI', 'ML'],
        ...     'Key Points': ['Point1', 'Point2'],
        ...     'Key Questions': [{'question': 'Q?', 'answer': 'A'}],
        ...     'Key Recommendations': ['Rec1'],
        ...     'Key Risks': ['Risk1'],
        ...     'Key Assumptions': ['Assumption1']
        ... }
        >>> summary = DocumentSummary.parse_obj(data)
        >>> print(summary.authors)
        ['Alice', 'Bob']
    """

    model_config = ConfigDict(populate_by_name=True)

    authors: list[str] = Field(
        default_factory=list,
        alias="Author",
        description="Who is the author of the document?",
    )
    title: str = Field(
        default="",
        alias="Title",
        description="What is the title of the document?",
    )
    date: str | None = Field(
        default=None, alias="Date", description="What is the date of the document?"
    )
    publisher: str | None = Field(
        default=None,
        alias="Publisher",
        description="Who is the publisher of the document?",
    )
    document_type: str = Field(
        default="other",
        alias="Document Type",
        description="Type of document (one of: bank_broker_report, academic_research, whitepaper, blog_post, financial_statement, marketing_material, other)",
    )
    summary: str = Field(
        default="",
        alias="Summary",
        description="Write a less than 250 word summary of the text below.",
    )
    topic_timelessness: str = Field(
        default="",
        alias="Topic Timelessness",
        description="Is the topic timeless or time sensitive (e.g time sensitive: if it is about a current event, stock, or market, timeless: if it is about a fundamental concept, theory, or idea)?",
    )
    keywords: list[str] = Field(
        default_factory=list,
        alias="Keywords",
        description="Write a list of up to 5 key keywords or themes from the document.",
    )
    key_points: list[str] = Field(
        default_factory=list,
        alias="Key Points",
        description="Write a list of the key points and takeaways from the document.",
    )
    key_questions: list[QuestionAnswer] = Field(
        default_factory=list,
        alias="Key Questions",
        description='Write a list of the key questions in the document and answers to those questions. For each question, include the question and the answer in a json format {{"question": <Question>, "answer": <Answer>}}.',
    )
    key_recommendations: list[str] = Field(
        default_factory=list,
        alias="Key Recommendations",
        description="Write a list of the key recommendations from the document.",
    )
    key_risks: list[str] = Field(
        default_factory=list,
        alias="Key Risks",
        description="Write a list of the key risks from the document.",
    )
    key_assumptions: list[str] = Field(
        default_factory=list,
        alias="Key Assumptions",
        description="Write a list of the key assumptions from the document.",
    )

    @field_validator("authors", mode="before")
    def ensure_authors_list(cls, v):
        if isinstance(v, str):
            return [v]
        return v

    @field_validator("date", mode="before")
    def ensure_date(cls, v):
        if v is None:
            return ""
        return v

    @field_validator("publisher", mode="before")
    def ensure_publisher(cls, v):
        if v is None:
            return ""
        return v


async def main():
    load_dotenv() 

    cfg = AppConfig(
        post_prompt="Summarise the document using the following model: {{response_model}}",
        response_model="examples.post_processing_example.DocumentSummary",
        use_cache=False,
    )
    pdf_path = Path(
        "/Users/joneickmeier/Documents/Papers Library/JFDS-2025-Varlashova-jfds.2025.1.191.pdf"
    )  # Replace with your PDF
    parser = AppConfig.from_path(pdf_path, cfg)
    result = await parser.parse(pdf_path)
    # print("Primary content length:", len(result.content))
    if result.post_content:
        # If the post-processed content is a Pydantic model, leverage its
        # built-in JSON serialisation for a nicely formatted, indented output.
        from pydantic import BaseModel  # Local import to avoid unused-import warnings

        if isinstance(result.post_content, BaseModel):
            print(
                "\nPost-processed output:\n",
                result.post_content.model_dump_json(indent=2),
            )
        else:
            import json
            print(
                "\nPost-processed output:\n",
                json.dumps(result.post_content, indent=2, default=str),
            )



if __name__ == "__main__":
    asyncio.run(main())



================================================================================
File: examples/pptx/pptx_example.py
================================================================================

"""Example usage of the PptxParser.

This script generates a small sample PowerPoint file on the fly (to avoid
committing binary assets) and then parses it using the library. The resulting
Markdown is printed to stdout.
"""

# mypy: ignore-errors

from __future__ import annotations

import asyncio
from pathlib import Path

from pptx import Presentation
from pptx.util import Inches

from doc_parser.config import AppConfig


def _build_sample_pptx(path: Path) -> None:
    """Create a minimal two-slide PPTX for demonstration purposes."""
    prs = Presentation()

    # Slide 1 – title & bullet list
    slide_layout = prs.slide_layouts[1]  # Title & content
    slide = prs.slides.add_slide(slide_layout)
    slide.shapes.title.text = "Document Parser Demo"
    body = slide.shapes.placeholders[1].text_frame
    body.text = "Key Features"
    for line in [
        "Supports PDF, DOCX, XLSX, PPTX, HTML",
        "AI-powered extraction",
        "Smart caching",
    ]:
        p = body.add_paragraph()
        p.text = line
        p.level = 1

    # Slide 2 – table example
    slide_layout = prs.slide_layouts[5]  # Title only
    slide = prs.slides.add_slide(slide_layout)
    slide.shapes.title.text = "Simple Table"

    rows, cols = 3, 3
    left, top, width, height = Inches(1), Inches(2), Inches(8), Inches(1.5)
    table_shape = slide.shapes.add_table(rows, cols, left, top, width, height)
    tbl = table_shape.table

    # Header row formatting
    headers = ["Col A", "Col B", "Col C"]
    for col, text in enumerate(headers):
        cell = tbl.cell(0, col)
        cell.text = text
        cell.text_frame.paragraphs[0].font.bold = True

    # Data rows
    for row in range(1, rows):
        for col in range(cols):
            tbl.cell(row, col).text = f"R{row}C{col}"

    prs.save(path)


async def _demo() -> None:
    # Ensure output directory exists and generate sample PPTX
    sample_path = Path("outputs/sample_demo.pptx")
    sample_path.parent.mkdir(parents=True, exist_ok=True)
    _build_sample_pptx(sample_path)

    # Configure parser
    cfg = AppConfig(output_format="markdown")
    parser = AppConfig.from_path(sample_path, cfg)

    # Parse the PPTX file
    result = await parser.parse(sample_path)

    # Print the resulting Markdown to stdout
    print(result.content)


if __name__ == "__main__":
    asyncio.run(_demo())



================================================================================
File: scripts/contatenate_code.py
================================================================================

"""Script to concatenate all source files for documentation purposes."""

import logging
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Define the paths to include (can be empty for 'include everything' mode)
include_paths = []  # [
#    "fin_statement_model",
#    "tests",
#    "examples",
#    # If you want to include everything, set this to []
# ]

# Define patterns to exclude
exclude_patterns = [
    "__pycache__",
    "*.pyc",
    ".git",
    ".pytest_cache",
    "*.egg-info",
    ".venv/**",
    ".ruff_cache/**",
    ".mypy_cache/**",
    ".nox/**",
    "dist/**",
    ".hypothesis/**",
]


def get_default_includes():
    """Return all dirs and files at top level if includes list is empty."""
    return [str(p) for p in Path(".").iterdir() if p.is_dir() or p.is_file()]


def is_excluded(path, patterns):
    """Check if a path matches any exclude pattern (very basic globbing)."""
    from fnmatch import fnmatch

    for pattern in patterns:
        # Match either by name (for dirs) or glob (for files)
        if fnmatch(path.name, pattern) or fnmatch(str(path), pattern):
            return True
    return False


def concatenate_files(output_filename="concatenated_code.txt"):
    """Concatenate all Python files in the specified directories."""
    file_paths = []

    # If no includes specified, include everything at the top level
    paths_to_include = include_paths or get_default_includes()

    for path_str in paths_to_include:
        path = Path(path_str)
        if path.exists():
            # Only descend into dirs, or add the file directly
            if path.is_dir():
                for py_file in path.rglob("*.py"):
                    if not is_excluded(py_file, exclude_patterns):
                        file_paths.append(py_file)
            elif path.is_file() and path.suffix == ".py":
                if not is_excluded(path, exclude_patterns):
                    file_paths.append(path)

    file_paths.sort()

    with open(output_filename, "w", encoding="utf-8") as output_file:
        for file_path in file_paths:
            output_file.write(f"\n{'=' * 80}\n")
            output_file.write(f"File: {file_path}\n")
            output_file.write(f"{'=' * 80}\n\n")

            try:
                with open(file_path, encoding="utf-8") as input_file:
                    content = input_file.read()
                    output_file.write(content)
                    output_file.write("\n\n")
            except FileNotFoundError:
                logger.warning("File not found at %s", file_path.resolve())
            except UnicodeDecodeError:
                logger.warning("Could not decode file %s as UTF-8; skipping", file_path)
            except Exception as e:
                logger.warning("Error reading file %s: %s", file_path, e)

    try:
        output_path = Path(output_filename)
        if output_path.exists():
            logger.info(
                f"Successfully concatenated {len(file_paths)} files into {output_filename}"
            )
    except Exception as e:
        logger.error("Error writing to output file %s: %s", output_filename, e)


if __name__ == "__main__":
    concatenate_files()



================================================================================
File: tests/cli/test_cli.py
================================================================================

from pathlib import Path

from typer.testing import CliRunner

from doc_parser import cli as dp_cli
from doc_parser.config import AppConfig
from doc_parser.core.base import BaseParser, ParseResult
from pydantic import BaseModel

# Ensure Path is available in annotations for Typer introspection
dp_cli.Path = Path  # type: ignore[attr-defined]

app = dp_cli.app

# Register a lightweight parser for .txt (if not already registered)
@AppConfig.register("txt_cli", [".txt"])
class TxtCliParser(BaseParser):
    async def validate_input(self, input_path: Path) -> bool:  # noqa: D401
        return True

    async def _parse(self, input_path: Path, *, options: BaseModel | None = None):  # noqa: D401
        _ = options
        return ParseResult(content=input_path.read_text(), metadata={})


def test_cli_parse_markdown(tmp_path):
    sample = tmp_path / "sample.txt"
    sample.write_text("Hello")

    runner = CliRunner()
    result = runner.invoke(app, [str(sample), "-f", "markdown"])
    assert result.exit_code == 0
    assert "Hello" in result.output 


================================================================================
File: tests/core/test_base_parser_additional.py
================================================================================

import asyncio
from pathlib import Path
from typing import Any
from pydantic import BaseModel

import pytest

from doc_parser.core.base import BaseParser, ParseResult
from doc_parser.config import AppConfig


class CountingParser(BaseParser):
    """Dummy parser that counts how many times _parse is executed."""

    def __init__(self, settings: AppConfig):
        super().__init__(settings)
        self.call_count = 0

    async def validate_input(self, input_path: Path) -> bool:  # noqa: D401
        return True

    async def _parse(self, input_path: Path, *, options: BaseModel | None = None) -> ParseResult:  # noqa: D401
        _ = options
        self.call_count += 1
        return ParseResult(content="dummy", metadata=self.get_metadata(input_path), output_format=self.settings.output_format)


@pytest.mark.asyncio
async def test_parse_markdown_and_json_wrappers(tmp_path):
    file_path = tmp_path / "file.txt"
    file_path.write_text("x")

    parser = CountingParser(AppConfig(use_cache=False))

    md_result = await parser.parse_markdown(file_path)
    assert md_result.content == "dummy"
    assert md_result.output_format == "markdown"

    json_result = await parser.parse_json(file_path)
    assert json_result.content == "dummy"
    assert json_result.output_format == "json"


@pytest.mark.asyncio
async def test_parse_caching(tmp_path):
    file_path = tmp_path / "f.txt"
    file_path.write_text("y")
    settings = AppConfig(use_cache=True, cache_dir=tmp_path / "cache")
    parser = CountingParser(settings)

    # First parse stores in cache
    _ = await parser.parse(file_path)
    assert parser.call_count == 1
    # Second parse should hit cache (call_count unchanged)
    _ = await parser.parse(file_path)
    assert parser.call_count == 1


@pytest.mark.asyncio
async def test_post_processing_success(monkeypatch, tmp_path):
    file_path = tmp_path / "p.txt"
    file_path.write_text("x")

    # Dummy LLM that returns processed content
    class DummyLLM:
        def __init__(self, *_args, **_kwargs):
            pass

        async def process(self, _content: str, _prompt: str):  # noqa: D401
            return "processed"

    # Patch the real LLMPostProcessor with our dummy
    import doc_parser.utils.llm_post_processor as lpp  # noqa: WPS433

    monkeypatch.setattr(lpp, "LLMPostProcessor", DummyLLM, raising=True)

    settings = AppConfig(use_cache=False, post_prompt="Summarize")
    parser = CountingParser(settings)

    result = await parser.parse(file_path)
    assert result.post_content == "processed"
    assert result.errors == []


@pytest.mark.asyncio
async def test_post_processing_failure(monkeypatch, tmp_path):
    file_path = tmp_path / "p2.txt"
    file_path.write_text("x")

    class FailingLLM:
        def __init__(self, *_args, **_kwargs):
            pass

        async def process(self, *_a, **_kw):  # noqa: D401, ANN001
            raise RuntimeError("boom")

    import doc_parser.utils.llm_post_processor as lpp  # noqa: WPS433

    monkeypatch.setattr(lpp, "LLMPostProcessor", FailingLLM, raising=True)

    settings = AppConfig(use_cache=False, post_prompt="Prompt")
    parser = CountingParser(settings)

    result = await parser.parse(file_path)
    assert result.post_content is None
    assert any("Post-processing failed" in e for e in result.errors) 


================================================================================
File: tests/core/test_base_parser_helpers.py
================================================================================

from pathlib import Path
from typing import Any
from pydantic import BaseModel

from doc_parser.core.base import BaseParser, ParseResult
from doc_parser.config import AppConfig


class DummyParser(BaseParser):
    async def validate_input(self, input_path: Path) -> bool:  # noqa: D401
        return True

    async def _parse(self, input_path: Path, *, options: BaseModel | None = None) -> ParseResult:  # noqa: D401
        _ = options
        return ParseResult(content="dummy", metadata=self.get_metadata(input_path))


def test_generate_cache_key(tmp_path):
    file_path = tmp_path / "f.txt"
    file_path.write_text("x")

    parser = DummyParser(AppConfig())
    key1 = parser.generate_cache_key(file_path)

    # Modify file to change mtime
    file_path.write_text("y")
    key2 = parser.generate_cache_key(file_path)

    assert key1 != key2


def test_parse_result_helpers(tmp_path):
    pr = ParseResult(content="abc", metadata={"a": 1})

    # to_dict roundtrip
    d = pr.to_dict()
    assert d["content"] == "abc"

    # to_json returns valid JSON string
    js = pr.to_json()
    assert "\"content\": \"abc\"" in js

    # save_markdown writes to disk
    md_path = tmp_path / "out.md"
    pr.save_markdown(md_path)
    assert md_path.read_text() == "abc" 


================================================================================
File: tests/core/test_registry_and_settings.py
================================================================================

from pathlib import Path
from typing import Any
from pydantic import BaseModel

import pytest

from doc_parser.core.base import BaseParser, ParseResult
from doc_parser.config import AppConfig
from doc_parser.core.exceptions import UnsupportedFormatError


# ---------------------------------------------------------------------------
# Dummy parser for .txt files (simple in-memory implementation)
# ---------------------------------------------------------------------------

@AppConfig.register("txt", [".txt"])
class TxtParser(BaseParser):
    async def validate_input(self, input_path: Path) -> bool:  # noqa: D401
        return input_path.suffix.lower() == ".txt"

    async def _parse(self, input_path: Path, *, options: BaseModel | None = None) -> ParseResult:  # noqa: D401
        _ = options
        return ParseResult(content=input_path.read_text(), metadata=self.get_metadata(input_path))


# ---------------------------------------------------------------------------
# Registry behaviour
# ---------------------------------------------------------------------------

def test_registry_from_path(tmp_path):
    path = tmp_path / "sample.txt"
    path.write_text("hello")

    parser = AppConfig.from_path(path)
    assert isinstance(parser, TxtParser)


def test_registry_list_and_support(tmp_path):
    # list_parsers should include our 'txt' registration
    parsers_map = AppConfig.list_parsers()
    assert "txt" in parsers_map
    assert ".txt" in parsers_map["txt"]

    # is_supported should reflect that
    txt_file = tmp_path / "file.txt"
    txt_file.write_text("x")
    assert AppConfig.is_supported(txt_file) is True

    # Unsupported extension raises
    unsupported = tmp_path / "file.xyz"
    unsupported.write_text("x")
    with pytest.raises(UnsupportedFormatError):
        AppConfig.from_path(unsupported)


def test_registry_get_parser_by_name():
    parser = AppConfig.get_parser_by_name("txt")
    assert isinstance(parser, TxtParser)


# ---------------------------------------------------------------------------
# Settings model helpers
# ---------------------------------------------------------------------------

def test_settings_path_coercion(tmp_path):
    cache_dir = tmp_path / "cache"
    out_dir = tmp_path / "out"

    settings = AppConfig(cache_dir=str(cache_dir), output_dir=str(out_dir))

    # Directories should have been created and typed as Path
    assert isinstance(settings.cache_dir, Path) and settings.cache_dir.exists()
    assert isinstance(settings.output_dir, Path) and settings.output_dir.exists()


def test_settings_parser_cfg():
    overrides = {"excel": {"include_formulas": True}}
    settings = AppConfig(parser_settings=overrides)
    assert settings.parser_cfg("excel") == overrides["excel"] 


================================================================================
File: tests/parsers/test_docx_parser_integration.py
================================================================================

import asyncio
from pathlib import Path

import docx
import pytest

from doc_parser.config import AppConfig
from doc_parser.parsers.docx.parser import DocxParser


@pytest.fixture()
def make_sample_docx(tmp_path):
    """Return path to a dynamically generated small DOCX file."""

    def _factory() -> Path:
        file_path = tmp_path / "sample.docx"
        document = docx.Document()

        # Heading and paragraph
        document.add_heading("Title", level=1)
        document.add_paragraph("This is a paragraph.")

        # Table 2x2
        table = document.add_table(rows=2, cols=2)
        table.cell(0, 0).text = "H1"
        table.cell(0, 1).text = "H2"
        table.cell(1, 0).text = "A"
        table.cell(1, 1).text = "B"

        document.save(file_path)
        return file_path

    return _factory


@pytest.mark.asyncio
async def test_docx_parser_markdown(make_sample_docx):
    docx_path = make_sample_docx()

    settings = AppConfig(
        output_format="markdown",
        parser_settings={"docx": {"extract_images": False}},
    )
    parser = DocxParser(settings)

    result = await parser.parse(docx_path)
    assert "# Title" in result.content
    assert "This is a paragraph." in result.content
    # Table markdown header row
    assert "| H1 | H2 |" in result.content

    # metadata checks
    assert result.metadata["paragraphs"] >= 2
    assert result.metadata["tables"] == 1
    assert result.output_format == "markdown"


@pytest.mark.asyncio
async def test_docx_parser_json(make_sample_docx):
    docx_path = make_sample_docx()

    settings = AppConfig(output_format="json")
    parser = DocxParser(settings)

    result = await parser.parse(docx_path)
    assert "\"paragraphs\"" in result.content
    assert "\"tables\"" in result.content


def test_docx_validate_input_neg(tmp_path):
    fake_path = tmp_path / "not_docx.txt"
    fake_path.write_text("x")

    parser = DocxParser(AppConfig())
    assert asyncio.run(parser.validate_input(fake_path)) is False 


================================================================================
File: tests/parsers/test_excel_parser_integration.py
================================================================================

import asyncio
from pathlib import Path
from typing import Any

import openpyxl
import pytest

from doc_parser.config import AppConfig
from doc_parser.parsers.excel.parser import ExcelParser


@pytest.fixture()
def make_sample_excel(tmp_path):  # noqa: D401
    """Return path to a generated .xlsx workbook with small data."""

    def _factory() -> Path:
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = "Sheet1"
        ws["A1"].value = "Name"
        ws["B1"].value = "Age"
        ws.append(["Alice", 30])
        ws.append(["Bob", 25])

        file_path = tmp_path / "sample.xlsx"
        wb.save(file_path)
        return file_path

    return _factory


@pytest.mark.asyncio
async def test_excel_parser_markdown(make_sample_excel):
    xlsx_path = make_sample_excel()

    settings = AppConfig(output_format="markdown")
    parser = ExcelParser(settings)
    result = await parser.parse(xlsx_path)

    assert result.content.startswith("# Sheet: Sheet1")
    # Table header row should be present
    assert "| Name | Age |" in result.content
    # Metadata checks
    assert result.metadata["sheet_count"] == 1
    assert result.output_format == "markdown"


@pytest.mark.asyncio
async def test_excel_parser_json(make_sample_excel):
    xlsx_path = make_sample_excel()
    settings = AppConfig(output_format="json")
    parser = ExcelParser(settings)
    result = await parser.parse(xlsx_path)

    # JSON string should include sheet name and data keys
    assert "\"Sheet1\"" in result.content
    assert "\"data\"" in result.content


def test_excel_validate_input_neg(tmp_path):
    fake = tmp_path / "file.txt"
    fake.write_text("x")
    parser = ExcelParser(AppConfig())
    assert asyncio.run(parser.validate_input(fake)) is False 


================================================================================
File: tests/parsers/test_html_helpers.py
================================================================================

from bs4 import BeautifulSoup
import pytest
import asyncio

from doc_parser.config import AppConfig
from doc_parser.parsers.html.parser import HtmlParser


@pytest.fixture()
def html_parser() -> HtmlParser:  # noqa: D401
    return HtmlParser(AppConfig())


def test_extract_title_and_description(html_parser: HtmlParser):
    html_doc = """
    <html>
      <head>
        <title>Example Page</title>
        <meta name="description" content="Simple description" />
      </head>
      <body><h1>Heading</h1><p>Text</p></body>
    </html>
    """
    soup = BeautifulSoup(html_doc, "html.parser")
    assert html_parser._extract_title(soup) == "Example Page"  # noqa: SLF001
    assert html_parser._extract_description(soup) == "Simple description"  # noqa: SLF001


@pytest.mark.asyncio
async def test_format_as_markdown_general_page(html_parser: HtmlParser):
    content_data = {
        "title": "My Title",
        "description": "Desc",
        "content": "Some *markdown* content",
        "links": [{"text": "Google", "url": "https://google.com"}],
        "is_perplexity": False,
    }
    # Enable link inclusion
    html_parser.follow_links = True  # type: ignore[attr-defined]
    md = await html_parser._format_as_markdown(content_data, "https://example.com")  # noqa: SLF001
    # Basic assertions on markdown structure
    assert md.startswith("# My Title")
    assert "Some *markdown* content" in md
    assert "[Google](https://google.com)" in md 


================================================================================
File: tests/parsers/test_html_parser_additional.py
================================================================================

import asyncio
import plistlib
from pathlib import Path

import pytest
from bs4 import BeautifulSoup

from doc_parser.config import AppConfig
from doc_parser.parsers.html.parser import HtmlParser


@pytest.fixture()
def html_parser(tmp_path):  # noqa: D401
    return HtmlParser(AppConfig(cache_dir=tmp_path / "cache"))


# ---------------------------------------------------------------------------
# Title / description fallbacks
# ---------------------------------------------------------------------------


def test_extract_title_description_meta(html_parser):
    html = """
    <html><head>
      <meta property="og:title" content="OG TITLE"/>
      <meta property="og:description" content="OG DESC"/>
    </head><body></body></html>
    """
    soup = BeautifulSoup(html, "html.parser")
    assert html_parser._extract_title(soup) == "OG TITLE"  # noqa: SLF001
    assert html_parser._extract_description(soup) == "OG DESC"  # noqa: SLF001


# ---------------------------------------------------------------------------
# .webloc validation & extraction
# ---------------------------------------------------------------------------


def test_validate_and_extract_webloc(tmp_path, html_parser):
    # Build a minimal plist webloc file
    data = {"URL": "https://example.com"}
    raw = plistlib.dumps(data).decode()
    webloc = tmp_path / "link.webloc"
    webloc.write_text(raw)

    assert asyncio.run(html_parser.validate_input(webloc)) is True
    # Protected method usage
    extracted = asyncio.run(html_parser._extract_url(webloc))  # noqa: SLF001
    assert extracted == "https://example.com"


# ---------------------------------------------------------------------------
# format_as_markdown when follow_links False – links section must be absent
# ---------------------------------------------------------------------------

@pytest.mark.asyncio
async def test_format_markdown_no_links(html_parser):
    html_parser.follow_links = False  # ensure disabled
    data = {
        "title": "T",
        "description": "D",
        "content": "Text",
        "links": [{"text": "L", "url": "https://l"}],
        "images": [],
        "is_perplexity": False,
    }
    md = await html_parser._format_as_markdown(data, "https://h")  # noqa: SLF001
    assert "## Links" not in md


# ---------------------------------------------------------------------------
# parse() of local .url file with mocked fetch
# ---------------------------------------------------------------------------

@pytest.mark.asyncio
async def test_parse_local_url_file(tmp_path, monkeypatch):
    url_file = tmp_path / "sample.url"
    url_file.write_text("""[InternetShortcut]\nURL=https://host/page\n""")

    async def fake_fetch(self, url: str):  # noqa: D401, ARG002
        return {
            "title": "Title",
            "description": "Desc",
            "content": "Body",
            "links": [],
            "images": [],
            "is_perplexity": False,
            "content_type": "text/html",
        }

    monkeypatch.setattr(
        "doc_parser.parsers.html.parser.HtmlParser._fetch_and_parse", fake_fetch, raising=True
    )

    parser = HtmlParser(AppConfig())
    result = await parser.parse(url_file)
    assert result.metadata["url"].startswith("https://host")
    assert "Body" in result.content 


================================================================================
File: tests/parsers/test_html_parser_helpers_deep.py
================================================================================

import asyncio
from bs4 import BeautifulSoup
import pytest

from doc_parser.config import AppConfig
from doc_parser.parsers.html.parser import HtmlParser


@pytest.fixture()
def parser():
    return HtmlParser(AppConfig())


# ---------------------------------------------------------------------------
# Perplexity page parsing
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_parse_perplexity_page(parser):
    html = """
    <html><body>
        <h1 class="query">What is AI?</h1>
        <div class="answer">Artificial intelligence answer</div>
        <a class="source" href="https://example.com">Example</a>
        <li class="related">What is ML?</li>
    </body></html>
    """
    soup = BeautifulSoup(html, "html.parser")
    data = await parser._parse_perplexity_page(soup, "https://perplexity.ai/xyz")  # noqa: SLF001

    assert data["query"].startswith("What is AI")
    assert "Artificial intelligence" in data["answer"]
    assert data["sources"][0]["url"] == "https://example.com"
    assert "What is ML?" in data["related_questions"][0]


@pytest.mark.asyncio
async def test_format_markdown_perplexity(parser):
    content_data = {
        "title": "My Title",
        "query": "Question?",
        "answer": "42",
        "sources": [{"title": "Src", "url": "https://s.com"}],
        "related_questions": ["Related Q"],
        "is_perplexity": True,
        "content_type": "text/html",
    }
    md = await parser._format_as_markdown(content_data, "https://page")  # noqa: SLF001
    assert "## Query" in md and "## Answer" in md and "## Sources" in md
    assert "[Src](https://s.com)" in md


# ---------------------------------------------------------------------------
# General page parsing with links & images
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_parse_general_page_links_images(parser):
    parser.follow_links = True  # type: ignore[attr-defined]

    html = """
    <html><body>
      <main>
        <p>Hello world</p>
        <a href="https://link">L</a>
        <img src="img.png" alt="Alt" />
      </main>
    </body></html>
    """
    soup = BeautifulSoup(html, "html.parser")
    data = await parser._parse_general_page(soup, "https://host")  # noqa: SLF001

    assert "Hello world" in data["content"]
    assert data["links"][0]["url"] == "https://link"
    assert data["images"][0]["src"] == "img.png"

    md = await parser._format_as_markdown(data | {"title": "T", "description": "D", "is_perplexity": False}, "https://host")  # noqa: SLF001
    # Markdown should include link list if follow_links True
    assert "## Links" in md 


================================================================================
File: tests/parsers/test_html_parser_integration.py
================================================================================

import asyncio

import pytest

from doc_parser.config import AppConfig
from doc_parser.parsers.html.parser import HtmlParser


@pytest.fixture()
def html_parser_full(tmp_path):  # noqa: D401
    settings = AppConfig(use_cache=False, cache_dir=tmp_path / "cache")
    return HtmlParser(settings)


# ---------------------------------------------------------------------------
# Local .url validation & extraction helpers
# ---------------------------------------------------------------------------


def test_validate_and_extract_url(tmp_path, html_parser_full):
    url_file = tmp_path / "link.url"
    url_file.write_text("""[InternetShortcut]\nURL=https://example.com\n""")

    # validate_input should return True
    assert asyncio.run(html_parser_full.validate_input(url_file)) is True

    # _extract_url should return the URL string (protected method)
    extracted = asyncio.run(html_parser_full._extract_url(url_file))  # noqa: SLF001
    assert extracted == "https://example.com"


# ---------------------------------------------------------------------------
# parse() with mocked _fetch_and_parse to avoid HTTP
# ---------------------------------------------------------------------------

@pytest.mark.asyncio
async def test_parse_url_with_mock(monkeypatch, html_parser_full):
    fake_data = {
        "title": "Mock Page",
        "description": "Desc",
        "content": "Body text",
        "links": [],
        "images": [],
        "is_perplexity": False,
        "content_type": "text/html",
    }

    async def fake_fetch(self, url: str):  # noqa: D401, ARG002
        return fake_data

    monkeypatch.setattr(
        "doc_parser.parsers.html.parser.HtmlParser._fetch_and_parse", fake_fetch, raising=True
    )

    result = await html_parser_full.parse("https://mock.page")
    assert result.metadata["title"] == "Mock Page"
    assert "Body text" in result.content 


================================================================================
File: tests/parsers/test_pdf_extractors.py
================================================================================

import asyncio
from types import SimpleNamespace

import pytest
from PIL import Image

from doc_parser.parsers.pdf.extractors import VisionExtractor
from doc_parser.prompts import PromptTemplate
from pydantic import BaseModel


@pytest.fixture()
def blank_image():  # noqa: D401
    return Image.new("RGB", (10, 10), color="white")


# ---------------------------------------------------------------------------
# Prompt helpers
# ---------------------------------------------------------------------------


def test_get_default_prompt(monkeypatch):
    """Ensure get_default_prompt returns expected text when template file is patched."""

    extractor = VisionExtractor()

    # Patch the method to avoid filesystem I/O in unit test
    monkeypatch.setattr(extractor, "get_default_prompt", lambda: "PROMPT", raising=True)

    assert extractor.get_default_prompt() == "PROMPT"


def test_get_prompt_resolution(monkeypatch):
    extractor = VisionExtractor()

    # Case 1: None -> default
    monkeypatch.setattr(extractor, "get_default_prompt", lambda: "DEFAULT")
    assert extractor._get_prompt(None) == "DEFAULT"  # noqa: SLF001

    # Case 2: PromptTemplate instance with str.format placeholder
    class XInput(BaseModel):
        x: str = "X"

    pt = PromptTemplate(template="HELLO {x}", input_schema=XInput)
    assert extractor._get_prompt(pt) == "HELLO X"

    rendered = pt.render({"x": "Y"})
    assert rendered == "HELLO Y"

    # Case 3: string treated as literal prompt
    assert extractor._get_prompt("LITERAL") == "LITERAL"


# ---------------------------------------------------------------------------
# Extraction helpers
# ---------------------------------------------------------------------------

@pytest.mark.asyncio
async def test_extract_single(monkeypatch, blank_image):
    extractor = VisionExtractor()

    async def fake_call(prompt: str, base64_str: str):  # noqa: D401, ARG002
        assert prompt == "PROMPT"
        return "RESULT"

    monkeypatch.setattr(extractor, "_call_vision_api", fake_call, raising=True)
    monkeypatch.setattr(extractor, "get_default_prompt", lambda: "PROMPT")

    output = await extractor.extract(blank_image)
    assert output == "RESULT"


@pytest.mark.asyncio
async def test_extract_batch(monkeypatch, blank_image):
    extractor = VisionExtractor()

    async def fake_single(self, img, prompt_template=None):  # noqa: ARG002, D401
        return "PAGE"

    monkeypatch.setattr(extractor, "_extract_single", fake_single, raising=True)

    images = [blank_image, blank_image]
    combined = await extractor.extract(images)
    # should join pages with double newline
    assert combined == "PAGE\n\nPAGE"


# ---------------------------------------------------------------------------
# _call_vision_api – patch Runner.run to avoid network
# ---------------------------------------------------------------------------

@pytest.mark.asyncio
async def test_call_vision_api(monkeypatch):
    extractor = VisionExtractor()

    # Create dummy result object returned by Runner.run
    dummy = SimpleNamespace(final_output="VISION")

    async def fake_run(agent, messages):  # noqa: D401, ARG002
        return dummy

    monkeypatch.setattr("doc_parser.parsers.pdf.extractors.Runner.run", fake_run, raising=True)

    out = await extractor._call_vision_api("PROMPT", "base64")
    assert out == "VISION" 


================================================================================
File: tests/parsers/test_pdf_helpers.py
================================================================================

from doc_parser.config import AppConfig
from doc_parser.parsers.pdf.parser import PDFParser


def test_combine_results():
    pdf_parser = PDFParser(AppConfig())
    inputs = ["Line 1", "", "Line 2", "", "", "Line 3"]
    combined = pdf_parser._combine_results(inputs)  # noqa: SLF001
    # Should join with single blank lines between distinct blocks, no leading/trailing multiples
    expected = "Line 1\n\nLine 2\n\nLine 3"
    assert combined == expected 


================================================================================
File: tests/parsers/test_pdf_parser_integration.py
================================================================================

import asyncio
from pathlib import Path
from typing import Any

import pytest
from PIL import Image

from doc_parser.config import AppConfig
from doc_parser.parsers.pdf.parser import PDFParser
from doc_parser.options import PdfOptions


@pytest.mark.asyncio
async def test_pdf_parser_parse_and_cache(tmp_path, monkeypatch):
    """Parse a dummy PDF (mocked) and verify caching behaviour."""
    # ------------------------------------------------------------------
    # 1. Create a fake PDF file (non-empty so validate_input passes)
    # ------------------------------------------------------------------
    pdf_path = tmp_path / "sample.pdf"
    pdf_path.write_text("dummy")

    # ------------------------------------------------------------------
    # 2. Patch pdf2image.convert_from_path to return fake images
    # ------------------------------------------------------------------
    def fake_convert(_path: str, **_kwargs: Any):  # noqa: D401
        # Always return two blank images regardless of kwargs
        return [Image.new("RGB", (10, 10), color="white") for _ in range(2)]

    monkeypatch.setattr(
        "doc_parser.parsers.pdf.parser.convert_from_path", fake_convert, raising=True
    )

    # ------------------------------------------------------------------
    # 3. Patch VisionExtractor.extract so we avoid external API calls
    # ------------------------------------------------------------------
    call_counter: dict[str, int] = {"count": 0}

    async def fake_extract(self, content, prompt_template=None):  # noqa: D401, ARG002
        call_counter["count"] += 1
        if isinstance(content, list):
            # Batch mode -> return numbered pages
            return "\n\n".join(f"Page {i+1}" for i in range(len(content)))
        return "Page 1"

    monkeypatch.setattr(
        "doc_parser.parsers.pdf.parser.VisionExtractor.extract", fake_extract, raising=True
    )

    # ------------------------------------------------------------------
    # 4. Run parser twice – second run should hit cache (extract not called)
    # ------------------------------------------------------------------
    settings = AppConfig(use_cache=True, cache_dir=tmp_path / "cache")
    parser = PDFParser(settings)

    result1 = await parser.parse(pdf_path)
    assert result1.metadata["pages"] == 2
    assert result1.content.startswith("Page 1")

    # Second run – VisionExtractor.extract should *not* be invoked again
    result2 = await parser.parse(pdf_path)
    assert result2.content == result1.content


@pytest.mark.asyncio
async def test_pdf_parser_page_range(tmp_path, monkeypatch):
    """Ensure page_range option limits conversion calls."""
    pdf_path = tmp_path / "range.pdf"
    pdf_path.write_text("x")

    # Fake convert_from_path counts pages requested
    captured_kwargs: dict[str, Any] = {}

    def fake_convert(_path: str, **kwargs: Any):  # noqa: D401
        captured_kwargs.update(kwargs)
        return [Image.new("RGB", (10, 10), color="white")]

    monkeypatch.setattr(
        "doc_parser.parsers.pdf.parser.convert_from_path", fake_convert, raising=True
    )

    async def fake_extract(self, content, prompt_template=None):  # noqa: D401, ARG002
        return "Single Page"

    monkeypatch.setattr(
        "doc_parser.parsers.pdf.parser.VisionExtractor.extract", fake_extract, raising=True
    )

    parser = PDFParser(AppConfig(use_cache=False))
    opts = PdfOptions(page_range=(1, 1))
    _ = await parser.parse(pdf_path, options=opts)

    # convert_from_path should have received first_page / last_page args
    assert captured_kwargs.get("first_page") == 1
    assert captured_kwargs.get("last_page") == 1 


================================================================================
File: tests/parsers/test_pptx_parser.py
================================================================================

"""Unit tests for PptxParser."""

from __future__ import annotations

from pathlib import Path

import pytest
from pptx import Presentation  # type: ignore
from pptx.util import Inches

from doc_parser.config import AppConfig

# mypy: ignore-errors

# Mark entire module as asyncio-compatible, allowing removal of individual decorators
pytestmark = pytest.mark.asyncio


def _create_temp_pptx(tmp_path: Path) -> Path:
    pptx_path = tmp_path / "temp.pptx"
    prs = Presentation()

    # Slide with title
    slide = prs.slides.add_slide(prs.slide_layouts[0])
    slide.shapes.title.text = "Test Slide"

    # Simple table
    rows, cols = 2, 2
    table_shape = slide.shapes.add_table(
        rows, cols, Inches(1), Inches(2), Inches(6), Inches(1)
    )
    tbl = table_shape.table
    tbl.cell(0, 0).text = "Header A"
    tbl.cell(0, 1).text = "Header B"
    tbl.cell(1, 0).text = "Val 1"
    tbl.cell(1, 1).text = "Val 2"

    prs.save(pptx_path)
    return pptx_path


@pytest.mark.asyncio
async def test_pptx_parser_markdown(tmp_path: Path) -> None:
    pptx_path = _create_temp_pptx(tmp_path)

    cfg = AppConfig(output_format="markdown", use_cache=False)
    parser = AppConfig.from_path(pptx_path, cfg)

    result = await parser.parse(pptx_path)

    assert not result.errors
    assert "# Test Slide" in result.content
    assert "| Header A" in result.content  # table header


@pytest.mark.asyncio
async def test_pptx_parser_json(tmp_path: Path) -> None:
    pptx_path = _create_temp_pptx(tmp_path)

    cfg = AppConfig(output_format="json", use_cache=False)
    parser = AppConfig.from_path(pptx_path, cfg)

    result = await parser.parse(pptx_path)

    assert not result.errors
    import json as _json

    data = _json.loads(result.content)
    assert isinstance(data, list)
    assert data[0]["title"] == "Test Slide"
    assert any("Header A" in cell for row in data[0]["tables"][0] for cell in row)



================================================================================
File: tests/parsers/test_pptx_parser_integration.py
================================================================================

import asyncio
from pathlib import Path

import pytest
from pptx import Presentation
from pptx.util import Inches

from doc_parser.config import AppConfig
from doc_parser.parsers.pptx.parser import PptxParser


@pytest.fixture()
def make_sample_pptx(tmp_path):  # noqa: D401
    """Return path to a generated PPTX with simple content."""

    def _factory() -> Path:
        prs = Presentation()

        # Title slide
        title_slide_layout = prs.slide_layouts[0]
        slide = prs.slides.add_slide(title_slide_layout)
        slide.shapes.title.text = "Sample Deck"
        slide.placeholders[1].text = "Subtitle"

        # Content slide with bullet list
        bullet_layout = prs.slide_layouts[1]
        slide2 = prs.slides.add_slide(bullet_layout)
        body = slide2.shapes.placeholders[1].text_frame
        body.text = "First"
        body.add_paragraph().text = "Second"

        # Save
        path = tmp_path / "deck.pptx"
        prs.save(path)
        return path

    return _factory


@pytest.mark.asyncio
async def test_pptx_parser_markdown(make_sample_pptx):
    pptx_path = make_sample_pptx()

    settings = AppConfig(
        output_format="markdown",
        parser_settings={"pptx": {"extract_images": False, "slide_delimiter": "---"}},
    )
    parser = PptxParser(settings)

    result = await parser.parse(pptx_path)
    # Should contain title as markdown heading
    assert result.content.startswith("# Sample Deck")
    # bullet list captured
    assert "Second" in result.content
    # Slide delimiter present
    assert "---" in result.content
    assert result.metadata["slides"] == 2


@pytest.mark.asyncio
async def test_pptx_parser_json(make_sample_pptx):
    pptx_path = make_sample_pptx()
    settings = AppConfig(output_format="json", use_cache=False, parser_settings={"pptx": {"extract_images": False}})
    parser = PptxParser(settings)
    result = await parser.parse(pptx_path)
    assert "\"title\"" in result.content
    assert "\"text\"" in result.content


def test_pptx_validate_input_neg(tmp_path):
    fake = tmp_path / "file.txt"
    fake.write_text("x")
    parser = PptxParser(AppConfig())
    assert asyncio.run(parser.validate_input(fake)) is False 


================================================================================
File: tests/utils/test_async_helpers_and_batcher.py
================================================================================

import asyncio
from collections import Counter

import pytest

from doc_parser.utils.async_batcher import AsyncBatcher, RateLimiter


@pytest.mark.asyncio
async def test_run_with_retry_success_after_failures():
    attempts = Counter()

    async def flaky(x: int) -> int:
        attempts["count"] += 1
        # Fail the first two times
        if attempts["count"] < 3:
            raise RuntimeError("boom")
        return x

    result = await AsyncBatcher.run_with_retry(flaky, 5, max_retries=3, backoff_factor=0.01)
    assert result == 5
    # Should have attempted exactly 3 times (2 failures + 1 success)
    assert attempts["count"] == 3


@pytest.mark.asyncio
async def test_rate_limiter_max_concurrency():
    max_concurrent = 2
    limiter = RateLimiter(max_concurrent)
    active = 0
    peak = 0

    async def task():
        nonlocal active, peak
        async with limiter:
            active += 1
            peak = max(peak, active)
            await asyncio.sleep(0.05)
            active -= 1

    await asyncio.gather(*(task() for _ in range(6)))
    assert peak <= max_concurrent


@pytest.mark.asyncio
async def test_async_batcher_basic():
    async def process(batch):
        # Double every number
        await asyncio.sleep(0.01)
        return [i * 2 for i in batch]

    batcher = AsyncBatcher(batch_size=3, process_func=process, timeout=0.05)

    results = await asyncio.gather(*[batcher.add(i) for i in [1, 2, 3, 4]])
    assert results == [2, 4, 6, 8] 


================================================================================
File: tests/utils/test_cache_and_llm.py
================================================================================

import asyncio
from datetime import timedelta
from pathlib import Path

import pytest

from doc_parser.config import AppConfig
from doc_parser.utils.cache import CacheManager, cache_set, cache_get
from doc_parser.utils.llm_post_processor import LLMPostProcessor


@pytest.mark.asyncio
async def test_cache_manager_roundtrip(tmp_path):
    cm = CacheManager(Path(tmp_path), ttl=timedelta(seconds=5))
    await cache_set(cm, "key", {"value": 1})
    data = await cache_get(cm, "key")
    assert data == {"value": 1}


@pytest.mark.asyncio
async def test_llm_post_processor_basic(tmp_path):
    settings = AppConfig(use_cache=True, cache_dir=tmp_path)
    proc = LLMPostProcessor(settings)

    # First call should compute and cache and return non-empty string
    out1 = await proc.process("hello", "Summarize")
    assert isinstance(out1, str) and out1.strip()  # non-empty

    # Second call should hit cache (simulate by checking same object)
    out2 = await proc.process("hello", "Summarize")
    assert out1 == out2  # should hit cache and match first output 


================================================================================
File: tests/utils/test_helpers_functions.py
================================================================================

import pandas as pd
from pathlib import Path

from doc_parser.utils.format_helpers import rows_to_markdown, dataframe_to_markdown
from doc_parser.core.base import ParseResult


def test_rows_to_markdown_basic():
    rows = [["Name", "Age"], ["Alice", "30"], ["Bob", "25"]]
    md = rows_to_markdown(rows)
    # Header row and one data row should appear
    assert md.startswith("| Name")
    assert "| Alice | 30 |" in md


def test_rows_to_markdown_escape():
    rows = [["A|B", "C\nD"], ["1", "2"]]
    md = rows_to_markdown(rows)
    # Pipe should be escaped and newline replaced with space
    assert "A\\|B" in md
    assert "C D" in md


def test_dataframe_to_markdown():
    df = pd.DataFrame({"Col1": [1, 2], "Col2": ["a", "b"]})
    md = dataframe_to_markdown(df)
    # Table should start with a Markdown header row
    assert md.startswith("| ")
    # Data rows must be present
    assert "| 1 | a |" in md


def test_dataframe_to_markdown_empty():
    import pandas as pd  # local import to avoid global fixture interference
    empty_df = pd.DataFrame()
    md = dataframe_to_markdown(empty_df)
    assert md == "*No data*"


def test_save_markdown(tmp_path):
    content = "# Title\n\nSample paragraph"
    dest = tmp_path / "sample.md"
    pr = ParseResult(content=content, format="markdown")
    pr.save_markdown(dest)
    assert dest.read_text(encoding="utf-8") == content


# Removed is_supported_file tests since functionality is now handled in parser classes. 


================================================================================
File: tests/utils/test_llm_post_processor.py
================================================================================

# mypy: ignore-errors

import json

from doc_parser.config import AppConfig
from doc_parser.prompts import PromptTemplate
from doc_parser.utils.llm_post_processor import LLMPostProcessor

import pytest

# Ensure all tests run under an asyncio event loop when using pytest-asyncio
pytestmark = pytest.mark.asyncio


async def test_resolve_prompt_literal(tmp_path):
    cfg = AppConfig(cache_dir=tmp_path)
    pp = LLMPostProcessor(cfg)
    result = await pp.process("hello", "echo this")
    assert isinstance(result, str) and result.strip()  # non-empty string


async def test_resolve_prompt_template(tmp_path):
    """Verify caching when using a custom PromptTemplate instance."""
    try:
        from pydantic import BaseModel  # type: ignore
    except ImportError:
        return  # Skip if pydantic not available

    class EchoInput(BaseModel):
        text: str

    template = PromptTemplate(template="Return {text}", input_schema=EchoInput)

    cfg = AppConfig(cache_dir=tmp_path)
    pp = LLMPostProcessor(cfg)

    rendered = template.render({"text": "foo"})
    out1 = await pp.process("bar", rendered)
    out2 = await pp.process("bar", rendered)  # should hit cache

    assert out1 == out2


# Simple Pydantic model validation example
async def test_response_model_validation(tmp_path):
    try:
        from pydantic import BaseModel  # type: ignore
    except ImportError:
        return  # Skip if pydantic not available

    class Echo(BaseModel):
        value: str

    # Dump model to a temporary module
    module_path = tmp_path / "mymodel.py"
    module_path.write_text(
        "from pydantic import BaseModel\nclass Echo(BaseModel):\n    value: str\n"
    )

    cfg = AppConfig(cache_dir=tmp_path, response_model="mymodel:Echo", use_cache=False)

    # Manipulate sys.path so import works
    import sys, importlib  # noqa: E401

    sys.path.insert(0, str(tmp_path))
    importlib.invalidate_caches()

    pp = LLMPostProcessor(cfg)
    # Provide valid JSON that matches Echo schema
    result = await pp.process(json.dumps({"value": "hi"}), "Return {value}")
    assert hasattr(result, "value")
    # Ensure the value is a string, actual content may vary depending on live LLM response
    assert isinstance(result.value, str) and result.value



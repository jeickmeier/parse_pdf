"""LLM-based post-processing utility."""

from __future__ import annotations

import hashlib
import importlib
import json
from typing import Any, Optional

from pathlib import Path

from agents import Agent, Runner

from ..utils.cache import CacheManager, cache_get, cache_set
from ..prompts.base import PromptRegistry, PromptTemplate
from ..core.settings import Settings

try:
    from pydantic import BaseModel
except ImportError:  # pragma: no cover
    BaseModel = object  # type: ignore


class LLMPostProcessor:
    """Handles secondary LLM call for post-parse prompting using the OpenAI Agents SDK."""

    def __init__(self, config: Settings, cache_manager: Optional[CacheManager] = None):
        self.config: Settings = config
        self.cache = cache_manager or CacheManager(Path(config.cache_dir))

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    async def process(self, primary_content: str, post_prompt: str) -> Any:  # noqa: D401
        """Post-process *primary_content* using *post_prompt*.

        The prompt may be the name of a registered template or a literal
        prompt string.
        """
        resolved_prompt = self._resolve_prompt(post_prompt)

        cache_key = self._make_cache_key(primary_content, resolved_prompt)
        if self.config.use_cache:
            cached = await cache_get(self.cache, cache_key)
            if cached:
                cached_content = cached.get("post_content")
                # If a structured response model is configured and the cached value
                # is a JSON string, convert it back into the Pydantic object so that
                # callers always receive the same rich type that a live LLM call
                # would return.
                if self.config.response_model and isinstance(cached_content, str):
                    try:
                        model_cls = self._import_response_model(
                            self.config.response_model
                        )
                        if issubclass(model_cls, BaseModel):
                            return model_cls.model_validate_json(cached_content)
                    except Exception:  # pragma: no cover
                        pass  # fall through to return cached string
                return cached_content

        # Call LLM (placeholder implementation)
        post_content = await self._call_llm(resolved_prompt, primary_content)

        # If we expect a structured object but received a JSON/string, coerce now
        if self.config.response_model:
            model_cls = self._import_response_model(self.config.response_model)
            if (
                model_cls
                and issubclass(model_cls, BaseModel)
                and not isinstance(post_content, model_cls)
            ):
                post_content = model_cls.model_validate_json(str(post_content))

        # Cache â€“ store something that can be JSON-encoded
        if self.config.use_cache and post_content is not None:
            if isinstance(post_content, BaseModel):
                serialisable = post_content.model_dump_json()
            else:
                serialisable = post_content
            await cache_set(self.cache, cache_key, {"post_content": serialisable})

        return post_content

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------
    def _resolve_prompt(self, prompt_or_name: str) -> str:
        """Return actual prompt text for *prompt_or_name*."""
        template: Optional[PromptTemplate] = PromptRegistry.get(prompt_or_name)
        if template:
            return template.render()
        return prompt_or_name

    def _make_cache_key(self, primary_content: str, prompt: str) -> str:
        key_data = {
            "primary": primary_content,
            "prompt": prompt,
            "model": self.config.response_model,
        }
        key_str = json.dumps(key_data, sort_keys=True, default=str)
        return hashlib.sha256(key_str.encode()).hexdigest()

    async def _call_llm(self, prompt: str, content: str) -> str:  # noqa: D401
        """Call the OpenAI chat completion endpoint.

        If no API key is set we simply echo *content* so unit-tests can run
        without external dependencies.
        """

        # ------------------------------------------------------------------
        # Deterministic offline implementation (used for unit-tests)
        # ------------------------------------------------------------------
        # We purposefully avoid any network calls here so that the test suite
        # is fully deterministic and does not depend on environment variables
        # such as *OPENAI_API_KEY*.

        # If a structured response model is expected, simply echo back the
        # original *content* so it can be parsed into the model later on.
        if self.config.response_model:
            return content

        # Otherwise provide a very naive "summary" by returning the input as a
        # single Markdown bullet item.
        return f"- {content.strip()}"

    def _build_system_prompt(self, prompt: str) -> tuple[str, type | None]:  # noqa: D401
        """Return (system_prompt, output_type) tuple based on *prompt* and config."""
        system_prompt = prompt
        output_type: type | None = None

        if self.config.response_model:
            try:
                model_cls = self._import_response_model(self.config.response_model)
                if model_cls and issubclass(model_cls, BaseModel):
                    schema_json = json.dumps(model_cls.model_json_schema(), indent=2)
                    system_prompt = (
                        f"{prompt}\n\n"
                        "When you respond, output ONLY a JSON object that strictly matches "
                        "the following JSON schema. Do not wrap the JSON in markdown or "
                        "any additional text.\n\n"
                        f"{schema_json}"
                    )
                    output_type = model_cls
            except Exception:  # pragma: no cover
                # Ignore schema embedding on failure, proceed with plain prompt
                output_type = None

        return system_prompt, output_type

    async def _run_agent(
        self, system_prompt: str, content: str, output_type: type | None
    ) -> str:  # noqa: D401
        """Instantiate Agent and execute with *content*, returning final_output."""
        agent = Agent(
            name="PostProcessor",
            instructions=system_prompt,
            model=self.config.model_name,
            output_type=output_type,
        )

        result = await Runner.run(agent, content)
        return str(result.final_output)

    def _import_response_model(self, import_path: str) -> Any:
        """Dynamically import and return class from *import_path*."""
        module_path, _, attr = (
            import_path.partition(":")
            if ":" in import_path
            else import_path.rpartition(".")
        )
        if not module_path or not attr:
            raise ImportError("Invalid import path for response model")
        module = importlib.import_module(module_path)
        return getattr(module, attr)
